{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will\n",
    "* Cluster Wikipedia documents using k-means\n",
    "* Explore the role of random initialization on the quality of the clustering\n",
    "* Explore how results differ after changing the number of clusters\n",
    "* Evaluate clustering, both quantitatively and qualitatively\n",
    "\n",
    "When properly executed, clustering uncovers valuable insights from a set of unlabeled documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note to Amazon EC2 users**: To conserve memory, make sure to stop all the other notebooks before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function # to conform python 2.x print to python 3.x\n",
    "import turicreate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with text data, we must first convert the documents into numerical features. As in the first assignment, let's extract TF-IDF features for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = turicreate.SFrame('people_wiki.sframe/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki['tf_idf'] = turicreate.text_analytics.tf_idf(wiki['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of the assignment, we will use sparse matrices. Sparse matrices are matrices that have a small number of nonzero entries. A good data structure for sparse matrices would only store the nonzero entries to save space and speed up computation. SciPy provides a highly-optimized library for sparse matrices. Many matrix operations available for NumPy arrays are also available for SciPy sparse matrices.\n",
    "\n",
    "We first convert the TF-IDF column (in dictionary format) into the SciPy sparse matrix format. We included plenty of comments for the curious; if you'd like, you may skip the next block and treat the function as a black box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sframe_to_scipy(x, column_name):\n",
    "    '''\n",
    "    Convert a dictionary column of an SFrame into a sparse matrix format where\n",
    "    each (row_id, column_id, value) triple corresponds to the value of\n",
    "    x[row_id][column_id], where column_id is a key in the dictionary.\n",
    "       \n",
    "    Example\n",
    "    >>> sparse_matrix, map_key_to_index = sframe_to_scipy(sframe, column_name)\n",
    "    '''\n",
    "    assert type(x[column_name][0]) == dict, \\\n",
    "        'The chosen column must be dict type, representing sparse data.'\n",
    "    \n",
    "    # 1. Add a row number (id)\n",
    "    x = x.add_row_number()\n",
    "\n",
    "    # 2. Stack will transform x to have a row for each unique (row, key) pair.\n",
    "    x = x.stack(column_name, ['feature', 'value'])\n",
    "\n",
    "    # Map feature words to integers \n",
    "    unique_words = sorted(x['feature'].unique())\n",
    "    mapping = {word:i for i, word in enumerate(unique_words)}\n",
    "    x['feature_id'] = x['feature'].apply(lambda x: mapping[x])\n",
    "\n",
    "    # Create numpy arrays that contain the data for the sparse matrix.\n",
    "    row_id = np.array(x['id'])\n",
    "    col_id = np.array(x['feature_id'])\n",
    "    data = np.array(x['value'])\n",
    "    \n",
    "    width = x['id'].max() + 1\n",
    "    height = x['feature_id'].max() + 1\n",
    "    \n",
    "    # Create a sparse matrix.\n",
    "    mat = csr_matrix((data, (row_id, col_id)), shape=(width, height))\n",
    "    return mat, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 5min 4s, sys: 7.01 s, total: 5min 11s\nWall time: 1min 26s\n"
    }
   ],
   "source": [
    "%%time\n",
    "# The conversion will take about a minute or two.\n",
    "tf_idf, map_index_to_word = sframe_to_scipy(wiki, 'tf_idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above matrix contains a TF-IDF score for each of the 59071 pages in the data set and each of the 547979 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(59071, 547979)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize all vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the previous assignment, Euclidean distance can be a poor metric of similarity between documents, as it unfairly penalizes long articles. For a reasonable assessment of similarity, we should disregard the length information and use length-agnostic metrics, such as cosine distance.\n",
    "\n",
    "The k-means algorithm does not directly work with cosine distance, so we take an alternative route to remove length information: we normalize all vectors to be unit length. It turns out that Euclidean distance closely mimics cosine distance when all vectors are unit length. In particular, the squared Euclidean distance between any two vectors of length one is directly proportional to their cosine distance.\n",
    "\n",
    "We can prove this as follows. Let $\\mathbf{x}$ and $\\mathbf{y}$ be normalized vectors, i.e. unit vectors, so that $\\|\\mathbf{x}\\|=\\|\\mathbf{y}\\|=1$. Write the squared Euclidean distance as the dot product of $(\\mathbf{x} - \\mathbf{y})$ to itself:\n",
    "\\begin{align*}\n",
    "\\|\\mathbf{x} - \\mathbf{y}\\|^2 &= (\\mathbf{x} - \\mathbf{y})^T(\\mathbf{x} - \\mathbf{y})\\\\\n",
    "                              &= (\\mathbf{x}^T \\mathbf{x}) - 2(\\mathbf{x}^T \\mathbf{y}) + (\\mathbf{y}^T \\mathbf{y})\\\\\n",
    "                              &= \\|\\mathbf{x}\\|^2 - 2(\\mathbf{x}^T \\mathbf{y}) + \\|\\mathbf{y}\\|^2\\\\\n",
    "                              &= 2 - 2(\\mathbf{x}^T \\mathbf{y})\\\\\n",
    "                              &= 2(1 - (\\mathbf{x}^T \\mathbf{y}))\\\\\n",
    "                              &= 2\\left(1 - \\frac{\\mathbf{x}^T \\mathbf{y}}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|}\\right)\\\\\n",
    "                              &= 2\\left[\\text{cosine distance}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "This tells us that two **unit vectors** that are close in Euclidean distance are also close in cosine distance. Thus, the k-means algorithm (which naturally uses Euclidean distances) on normalized vectors will produce the same results as clustering using cosine distance as a distance metric.\n",
    "\n",
    "We import the [`normalize()` function](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) from scikit-learn to normalize all vectors to unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<59071x547979 sparse matrix of type '<class 'numpy.float64'>'\n\twith 10379283 stored elements in Compressed Sparse Row format>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "tf_idf = normalize(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement the k-means algorithm. First, we choose an initial set of centroids. A common practice is to choose randomly from the data points.\n",
    "\n",
    "**Note:** We specify a seed here, so that everyone gets the same answer. In practice, we highly recommend to use different seeds every time (for instance, by using the current timestamp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_centroids(data, k, seed=None):\n",
    "    '''Randomly choose k data points as initial centroids'''\n",
    "    if seed is not None: # useful for obtaining consistent results\n",
    "        np.random.seed(seed)\n",
    "    n = data.shape[0] # number of data points\n",
    "        \n",
    "    # Pick K indices from range [0, N).\n",
    "    rand_indices = np.random.randint(0, n, k)\n",
    "    \n",
    "    # Keep centroids as dense format, as many entries will be nonzero due to averaging.\n",
    "    # As long as at least one document in a cluster contains a word,\n",
    "    # it will carry a nonzero weight in the TF-IDF vector of the centroid.\n",
    "    centroids = data[rand_indices,:].toarray()\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initialization, the k-means algorithm iterates between the following two steps:\n",
    "1. Assign each data point to the closest centroid.\n",
    "$$\n",
    "z_i \\gets \\mathrm{argmin}_j \\|\\mu_j - \\mathbf{x}_i\\|^2\n",
    "$$\n",
    "2. Revise centroids as the mean of the assigned data points.\n",
    "$$\n",
    "\\mu_j \\gets \\frac{1}{n_j}\\sum_{i:z_i=j} \\mathbf{x}_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pseudocode, we iteratively do the following:\n",
    "```\n",
    "cluster_assignment = assign_clusters(data, centroids)\n",
    "centroids = revise_centroids(data, k, cluster_assignment)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we implement Step 1 of the main k-means loop above? First import `pairwise_distances` function from scikit-learn, which calculates Euclidean distances between rows of given arrays. See [this documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html) for more information.\n",
    "\n",
    "For the sake of demonstration, let's look at documents 100 through 102 as query documents and compute the distances between each of these documents and every other document in the corpus. In the k-means algorithm, we will have to compute pairwise distances between the set of centroids and the set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1.41000789 1.36894636]\n [1.40935215 1.41023886]\n [1.39855967 1.40890299]\n ...\n [1.41108296 1.39123646]\n [1.41022804 1.31468652]\n [1.39899784 1.41072448]]\n"
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Get the TF-IDF vectors for documents 100 through 102.\n",
    "queries = tf_idf[100:102,:]\n",
    "\n",
    "# Compute pairwise distances from every data point to each query vector.\n",
    "dist = pairwise_distances(tf_idf, queries, metric='euclidean')\n",
    "\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, `dist[i,j]` is assigned the distance between the `i`th row of `X` (i.e., `X[i,:]`) and the `j`th row of `Y` (i.e., `Y[j,:]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** For a moment, suppose that we initialize three centroids with the first 3 rows of `tf_idf`. Write code to compute distances from each of the centroids to all data points in `tf_idf`. Then find the distance between row 430 of `tf_idf` and the second centroid and save it to `dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students should write code here\n",
    "dis=pairwise_distances(tf_idf,tf_idf[:3],metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=dis[430][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pass\n"
    }
   ],
   "source": [
    "'''Test cell'''\n",
    "if np.allclose(dist, pairwise_distances(tf_idf[430,:], tf_idf[1,:])):\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Check your code again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Next, given the pairwise distances, we take the minimum of the distances for each data point. Fittingly, NumPy provides an `argmin` function. See [this documentation](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.argmin.html) for details.\n",
    "\n",
    "Read the documentation and write code to produce a 1D array whose i-th entry indicates the centroid that is the closest to the i-th data point. Use the list of distances from the previous checkpoint and save them as `distances`. The value 0 indicates closeness to the first centroid, 1 indicates closeness to the second centroid, and so forth. Save this array as `closest_cluster`.\n",
    "\n",
    "**Hint:** the resulting array should be as long as the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.        , 1.40775177, 1.38784582],\n       [1.40775177, 0.        , 1.39867641],\n       [1.38784582, 1.39867641, 0.        ],\n       ...,\n       [1.37070999, 1.40978937, 1.40616385],\n       [1.35214578, 1.41306211, 1.40869799],\n       [1.40799024, 1.41353429, 1.40903605]])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students should write code here\n",
    "closest_cluster=np.argmin(dis,axis=1)\n",
    "distances=dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "59071"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "len(closest_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pass\n"
    }
   ],
   "source": [
    "'''Test cell'''\n",
    "reference = [list(row).index(min(row)) for row in distances]\n",
    "if np.allclose(closest_cluster, reference):\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Check your code again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Let's put these steps together.  First, initialize three centroids with the first 3 rows of `tf_idf`. Then, compute distances from each of the centroids to all data points in `tf_idf`. Finally, use these distance calculations to compute cluster assignments and assign them to `cluster_assignment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students should write code here\n",
    "cluster_assignment=np.argmin(pairwise_distances(tf_idf,tf_idf[:3],metric='euclidean'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pass\n"
    }
   ],
   "source": [
    "if len(cluster_assignment)==59071 and \\\n",
    "   np.array_equal(np.bincount(cluster_assignment), np.array([23061, 10086, 25924])):\n",
    "    print('Pass') # count number of data points for each cluster\n",
    "else:\n",
    "    print('Check your code again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to fill in the blanks in this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(data, centroids):\n",
    "    \n",
    "    # Compute distances between each data point and the set of centroids:\n",
    "    # Fill in the blank (RHS only)\n",
    "    distances_from_centroids = pairwise_distances(data,centroids,metric='euclidean')   # YOUR CODE HERE\n",
    "    \n",
    "    # Compute cluster assignments for each data point:\n",
    "    # Fill in the blank (RHS only)\n",
    "    cluster_assignment = np.argmin(distances_from_centroids,axis=1)   # YOUR CODE HERE\n",
    "    \n",
    "    return cluster_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**. For the last time, let us check if Step 1 was implemented correctly. With rows 0, 2, 4, and 6 of `tf_idf` as an initial set of centroids, we assign cluster labels to rows 0, 10, 20, ..., and 90 of `tf_idf`. The resulting cluster labels should be `[0, 1, 1, 0, 0, 2, 0, 2, 2, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pass\n"
    }
   ],
   "source": [
    "if np.allclose(assign_clusters(tf_idf[0:100:10], tf_idf[0:8:2]), np.array([0, 1, 1, 0, 0, 2, 0, 2, 2, 1])):\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Check your code again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revising clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn to Step 2, where we compute the new centroids given the cluster assignments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy and NumPy arrays allow for filtering via Boolean masks. For instance, we filter all data points that are assigned to cluster 0 by writing\n",
    "```\n",
    "data[cluster_assignment==0,:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop intuition about filtering, let's look at a toy example consisting of 3 data points and 2 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1., 2., 0.],\n",
    "                 [0., 0., 0.],\n",
    "                 [2., 2., 0.]])\n",
    "centroids = np.array([[0.5, 0.5, 0.],\n",
    "                      [0., -0.5, 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assign these data points to the closest centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 1 0]\n"
    }
   ],
   "source": [
    "cluster_assignment = assign_clusters(data, centroids)\n",
    "print(cluster_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression `cluster_assignment==1` gives a list of Booleans that says whether each data point is assigned to cluster 1 or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([False,  True, False])"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "cluster_assignment==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise for cluster 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ True, False,  True])"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "cluster_assignment==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lieu of indices, we can put in the list of Booleans to pick and choose rows. Only the rows that correspond to a `True` entry will be retained.\n",
    "\n",
    "First, let's look at the data points (i.e., their values) assigned to cluster 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "data[cluster_assignment==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense since [0 0 0] is closer to [0 -0.5 0] than to [0.5 0.5 0].\n",
    "\n",
    "Now let's look at the data points assigned to cluster 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1., 2., 0.],\n       [2., 2., 0.]])"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "data[cluster_assignment==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this makes sense since these values are each closer to [0.5 0.5 0] than to [0 -0.5 0].\n",
    "\n",
    "Given all the data points in a cluster, it only remains to compute the mean. Use [np.mean()](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.mean.html). By default, the function averages all elements in a 2D array. To compute row-wise or column-wise means, add the `axis` argument. See the linked documentation for details. \n",
    "\n",
    "Use this function to average the data points in cluster 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1.5, 2. , 0. ])"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "data[cluster_assignment==0].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to complete this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revise_centroids(data, k, cluster_assignment):\n",
    "    new_centroids = []\n",
    "    for i in range(k):\n",
    "        # Select all data points that belong to cluster i. Fill in the blank (RHS only)\n",
    "        member_data_points = data[cluster_assignment==i]   # YOUR CODE HERE\n",
    "        # Compute the mean of the data points. Fill in the blank (RHS only)\n",
    "        centroid = np.mean(member_data_points,axis=0)   # YOUR CODE HERE\n",
    "        \n",
    "        # Convert numpy.matrix type to numpy.ndarray type\n",
    "        centroid = centroid.A1\n",
    "        new_centroids.append(centroid)\n",
    "    new_centroids = np.array(new_centroids)\n",
    "    \n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**. Let's check our Step 2 implementation. Letting rows 0, 10, ..., 90 of `tf_idf` as the data points and the cluster labels `[0, 1, 1, 0, 0, 2, 0, 2, 2, 1]`, we compute the next set of centroids. Each centroid is given by the average of all member data points in corresponding cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pass\n"
    }
   ],
   "source": [
    "result = revise_centroids(tf_idf[0:100:10], 3, np.array([0, 1, 1, 0, 0, 2, 0, 2, 2, 1]))\n",
    "if np.allclose(result[0], np.mean(tf_idf[[0,30,40,60]].toarray(), axis=0)) and \\\n",
    "   np.allclose(result[1], np.mean(tf_idf[[10,20,90]].toarray(), axis=0))   and \\\n",
    "   np.allclose(result[2], np.mean(tf_idf[[50,70,80]].toarray(), axis=0)):\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Check your code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we tell if the k-means algorithm is converging? We can look at the cluster assignments and see if they stabilize over time. In fact, we'll be running the algorithm until the cluster assignments stop changing at all. To be extra safe, and to assess the clustering performance, we'll be looking at an additional criteria: the sum of all squared distances between data points and centroids. This is defined as\n",
    "$$\n",
    "J(\\mathcal{Z},\\mu) = \\sum_{j=1}^k \\sum_{i:z_i = j} \\|\\mathbf{x}_i - \\mu_j\\|^2.\n",
    "$$\n",
    "The smaller the distances, the more homogeneous the clusters are. In other words, we'd like to have \"tight\" clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_heterogeneity(data, k, centroids, cluster_assignment):\n",
    "    \n",
    "    heterogeneity = 0.0\n",
    "    for i in range(k):\n",
    "        \n",
    "        # Select all data points that belong to cluster i. Fill in the blank (RHS only)\n",
    "        member_data_points = data[cluster_assignment==i, :]\n",
    "        \n",
    "        if member_data_points.shape[0] > 0: # check if i-th cluster is non-empty\n",
    "            # Compute distances from centroid to data points (RHS only)\n",
    "            distances = pairwise_distances(member_data_points, [centroids[i]], metric='euclidean')\n",
    "            squared_distances = distances**2\n",
    "            heterogeneity += np.sum(squared_distances)\n",
    "        \n",
    "    return heterogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the cluster heterogeneity for the 2-cluster example we've been considering based on our current cluster assignments and centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "7.25"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "compute_heterogeneity(data, 2, centroids, cluster_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining into a single function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the two k-means steps have been implemented, as well as our heterogeneity metric we wish to monitor, it is only a matter of putting these functions together to write a k-means algorithm that\n",
    "\n",
    "* Repeatedly performs Steps 1 and 2\n",
    "* Tracks convergence metrics\n",
    "* Stops if either no assignment changed or we reach a certain number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the blanks\n",
    "def kmeans(data, k, initial_centroids, maxiter, record_heterogeneity=None, verbose=False):\n",
    "    '''This function runs k-means on given data and initial set of centroids.\n",
    "       maxiter: maximum number of iterations to run.\n",
    "       record_heterogeneity: (optional) a list, to store the history of heterogeneity as function of iterations\n",
    "                             if None, do not store the history.\n",
    "       verbose: if True, print how many data points changed their cluster labels in each iteration'''\n",
    "    centroids = initial_centroids[:]\n",
    "    prev_cluster_assignment = None\n",
    "    \n",
    "    for itr in range(maxiter):        \n",
    "        if verbose:\n",
    "            print(itr)\n",
    "        \n",
    "        # 1. Make cluster assignments using nearest centroids\n",
    "        # YOUR CODE HERE\n",
    "        cluster_assignment=np.argmin(pairwise_distances(data,centroids,metric='euclidean'),axis=1)\n",
    "            \n",
    "        # 2. Compute a new centroid for each of the k clusters, averaging all data points assigned to that cluster.\n",
    "        # YOUR CODE HERE\n",
    "        centroids = revise_centroids(data,k,cluster_assignment)\n",
    "        # Check for convergence: if none of the assignments changed, stop\n",
    "        if prev_cluster_assignment is not None and \\\n",
    "          (prev_cluster_assignment==cluster_assignment).all():\n",
    "            break\n",
    "        \n",
    "        # Print number of new assignments \n",
    "        if prev_cluster_assignment is not None:\n",
    "            num_changed = np.sum(prev_cluster_assignment!=cluster_assignment)\n",
    "            if verbose:\n",
    "                print('    {0:5d} elements changed their cluster assignment.'.format(num_changed))   \n",
    "        \n",
    "        # Record heterogeneity convergence metric\n",
    "        if record_heterogeneity is not None:\n",
    "            # YOUR CODE HERE\n",
    "            score = compute_heterogeneity(data,k,centroids,cluster_assignment)\n",
    "            record_heterogeneity.append(score)\n",
    "\n",
    "        prev_cluster_assignment = cluster_assignment[:]\n",
    "        \n",
    "    return centroids, cluster_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting convergence metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above function to plot the convergence metric across iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heterogeneity(heterogeneity, k):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(heterogeneity, linewidth=4)\n",
    "    plt.xlabel('# Iterations')\n",
    "    plt.ylabel('Heterogeneity')\n",
    "    plt.title('Heterogeneity of clustering over time, K={0:d}'.format(k))\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider running k-means with K=3 clusters for a maximum of 400 iterations, recording cluster heterogeneity at every step.  Then, let's plot the heterogeneity over iterations using the plotting function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n1\n    19157 elements changed their cluster assignment.\n2\n     7739 elements changed their cluster assignment.\n3\n     5119 elements changed their cluster assignment.\n4\n     3370 elements changed their cluster assignment.\n5\n     2811 elements changed their cluster assignment.\n6\n     3233 elements changed their cluster assignment.\n7\n     3815 elements changed their cluster assignment.\n8\n     3172 elements changed their cluster assignment.\n9\n     1149 elements changed their cluster assignment.\n10\n      498 elements changed their cluster assignment.\n11\n      265 elements changed their cluster assignment.\n12\n      149 elements changed their cluster assignment.\n13\n      100 elements changed their cluster assignment.\n14\n       76 elements changed their cluster assignment.\n15\n       67 elements changed their cluster assignment.\n16\n       51 elements changed their cluster assignment.\n17\n       47 elements changed their cluster assignment.\n18\n       40 elements changed their cluster assignment.\n19\n       34 elements changed their cluster assignment.\n20\n       35 elements changed their cluster assignment.\n21\n       39 elements changed their cluster assignment.\n22\n       24 elements changed their cluster assignment.\n23\n       16 elements changed their cluster assignment.\n24\n       12 elements changed their cluster assignment.\n25\n       14 elements changed their cluster assignment.\n26\n       17 elements changed their cluster assignment.\n27\n       15 elements changed their cluster assignment.\n28\n       14 elements changed their cluster assignment.\n29\n       16 elements changed their cluster assignment.\n30\n       21 elements changed their cluster assignment.\n31\n       22 elements changed their cluster assignment.\n32\n       33 elements changed their cluster assignment.\n33\n       35 elements changed their cluster assignment.\n34\n       39 elements changed their cluster assignment.\n35\n       36 elements changed their cluster assignment.\n36\n       36 elements changed their cluster assignment.\n37\n       25 elements changed their cluster assignment.\n38\n       27 elements changed their cluster assignment.\n39\n       25 elements changed their cluster assignment.\n40\n       28 elements changed their cluster assignment.\n41\n       35 elements changed their cluster assignment.\n42\n       31 elements changed their cluster assignment.\n43\n       25 elements changed their cluster assignment.\n44\n       18 elements changed their cluster assignment.\n45\n       15 elements changed their cluster assignment.\n46\n       10 elements changed their cluster assignment.\n47\n        8 elements changed their cluster assignment.\n48\n        8 elements changed their cluster assignment.\n49\n        8 elements changed their cluster assignment.\n50\n        7 elements changed their cluster assignment.\n51\n        8 elements changed their cluster assignment.\n52\n        3 elements changed their cluster assignment.\n53\n        3 elements changed their cluster assignment.\n54\n        4 elements changed their cluster assignment.\n55\n        2 elements changed their cluster assignment.\n56\n        3 elements changed their cluster assignment.\n57\n        3 elements changed their cluster assignment.\n58\n        1 elements changed their cluster assignment.\n59\n        1 elements changed their cluster assignment.\n60\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 504x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"268.399pt\" version=\"1.1\" viewBox=\"0 0 489.69175 268.399\" width=\"489.69175pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 268.399 \nL 489.69175 268.399 \nL 489.69175 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 84.585 219.229 \nL 465.7625 219.229 \nL 465.7625 27.789 \nL 84.585 27.789 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mfa2f358f51\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"101.91125\" xlink:href=\"#mfa2f358f51\" y=\"219.229\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(96.82125 238.3865)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"160.644301\" xlink:href=\"#mfa2f358f51\" y=\"219.229\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(150.464301 238.3865)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.377352\" xlink:href=\"#mfa2f358f51\" y=\"219.229\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(209.197352 238.3865)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"278.110403\" xlink:href=\"#mfa2f358f51\" y=\"219.229\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(267.930403 238.3865)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"336.843453\" xlink:href=\"#mfa2f358f51\" y=\"219.229\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(326.663453 238.3865)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"395.576504\" xlink:href=\"#mfa2f358f51\" y=\"219.229\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(385.396504 238.3865)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"454.309555\" xlink:href=\"#mfa2f358f51\" y=\"219.229\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(444.129555 238.3865)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- # Iterations -->\n     <defs>\n      <path d=\"M 51.125 44 \nL 36.921875 44 \nL 32.8125 27.6875 \nL 47.125 27.6875 \nz\nM 43.796875 71.78125 \nL 38.71875 51.515625 \nL 52.984375 51.515625 \nL 58.109375 71.78125 \nL 65.921875 71.78125 \nL 60.890625 51.515625 \nL 76.125 51.515625 \nL 76.125 44 \nL 58.984375 44 \nL 54.984375 27.6875 \nL 70.515625 27.6875 \nL 70.515625 20.21875 \nL 53.078125 20.21875 \nL 48 0 \nL 40.1875 0 \nL 45.21875 20.21875 \nL 30.90625 20.21875 \nL 25.875 0 \nL 18.015625 0 \nL 23.09375 20.21875 \nL 7.71875 20.21875 \nL 7.71875 27.6875 \nL 24.90625 27.6875 \nL 29 44 \nL 13.28125 44 \nL 13.28125 51.515625 \nL 30.90625 51.515625 \nL 35.890625 71.78125 \nz\n\" id=\"DejaVuSans-35\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-73\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(227.82625 257.8715)scale(0.16 -0.16)\">\n      <use xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"83.789062\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"115.576172\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"145.068359\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"184.277344\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"245.800781\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"286.914062\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"348.193359\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"387.402344\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"415.185547\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"476.367188\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"539.746094\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m351d35ca9a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"84.585\" xlink:href=\"#m351d35ca9a\" y=\"170.20512\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 58100 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(26.685 176.28387)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"84.585\" xlink:href=\"#m351d35ca9a\" y=\"110.64778\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 58200 -->\n      <g transform=\"translate(26.685 116.72653)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"84.585\" xlink:href=\"#m351d35ca9a\" y=\"51.090441\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 58300 -->\n      <g transform=\"translate(26.685 57.169191)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- Heterogeneity -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 43.015625 \nL 55.515625 43.015625 \nL 55.515625 72.90625 \nL 65.375 72.90625 \nL 65.375 0 \nL 55.515625 0 \nL 55.515625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-72\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <g transform=\"translate(19.3575 180.77525)rotate(-90)scale(0.16 -0.16)\">\n      <use xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"75.195312\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"136.71875\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"175.927734\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"237.451172\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"278.533203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"339.714844\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"403.191406\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"464.714844\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"528.09375\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"589.617188\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"617.400391\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"656.609375\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path clip-path=\"url(#p85d1cf45af)\" d=\"M 101.91125 36.490818 \nL 107.784555 119.648666 \nL 113.65786 147.075962 \nL 119.531165 160.947301 \nL 125.40447 167.141276 \nL 131.277775 171.679054 \nL 137.151081 177.856933 \nL 143.024386 189.547362 \nL 148.897691 207.0332 \nL 154.770996 209.943174 \nL 160.644301 210.224444 \nL 166.517606 210.328187 \nL 172.390911 210.373909 \nL 178.264216 210.397401 \nL 184.137521 210.414333 \nL 190.010826 210.428456 \nL 195.884131 210.437319 \nL 201.757436 210.445339 \nL 207.630742 210.450379 \nL 213.504047 210.454499 \nL 219.377352 210.45958 \nL 225.250657 210.463826 \nL 231.123962 210.466216 \nL 236.997267 210.467603 \nL 242.870572 210.468559 \nL 248.743877 210.469681 \nL 254.617182 210.471067 \nL 260.490487 210.4722 \nL 266.363792 210.473304 \nL 272.237097 210.474518 \nL 278.110403 210.476488 \nL 283.983708 210.478568 \nL 289.857013 210.482176 \nL 295.730318 210.486133 \nL 301.603623 210.489949 \nL 307.476928 210.493421 \nL 313.350233 210.497317 \nL 319.223538 210.500006 \nL 325.096843 210.502523 \nL 330.970148 210.504724 \nL 336.843453 210.507546 \nL 342.716758 210.511988 \nL 348.590064 210.515806 \nL 354.463369 210.518804 \nL 360.336674 210.520462 \nL 366.209979 210.521762 \nL 372.083284 210.522591 \nL 377.956589 210.523182 \nL 383.829894 210.523758 \nL 389.703199 210.524326 \nL 395.576504 210.524909 \nL 401.449809 210.525613 \nL 407.323114 210.525876 \nL 413.196419 210.526128 \nL 419.069725 210.526395 \nL 424.94303 210.526529 \nL 430.816335 210.526715 \nL 436.68964 210.526922 \nL 442.562945 210.527092 \nL 448.43625 210.527182 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:4;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 84.585 219.229 \nL 84.585 27.789 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 465.7625 219.229 \nL 465.7625 27.789 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 84.585 219.229 \nL 465.7625 219.229 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 84.585 27.789 \nL 465.7625 27.789 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_13\">\n    <!-- Heterogeneity of clustering over time, K=3 -->\n    <defs>\n     <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n     <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n     <path d=\"M 11.71875 12.40625 \nL 22.015625 12.40625 \nL 22.015625 4 \nL 14.015625 -11.625 \nL 7.71875 -11.625 \nL 11.71875 4 \nz\n\" id=\"DejaVuSans-44\"/>\n     <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 42.09375 \nL 52.390625 72.90625 \nL 65.09375 72.90625 \nL 28.90625 38.921875 \nL 67.671875 0 \nL 54.6875 0 \nL 19.671875 35.109375 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-75\"/>\n     <path d=\"M 10.59375 45.40625 \nL 73.1875 45.40625 \nL 73.1875 37.203125 \nL 10.59375 37.203125 \nz\nM 10.59375 25.484375 \nL 73.1875 25.484375 \nL 73.1875 17.1875 \nL 10.59375 17.1875 \nz\n\" id=\"DejaVuSans-61\"/>\n    </defs>\n    <g transform=\"translate(67.85575 21.789)scale(0.192 -0.192)\">\n     <use xlink:href=\"#DejaVuSans-72\"/>\n     <use x=\"75.195312\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"136.71875\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"175.927734\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"237.451172\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"278.533203\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"339.714844\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"403.191406\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"464.714844\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"528.09375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"589.617188\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"617.400391\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"656.609375\" xlink:href=\"#DejaVuSans-121\"/>\n     <use x=\"715.789062\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"747.576172\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"808.757812\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"843.962891\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"875.75\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"930.730469\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"958.513672\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"1021.892578\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1073.992188\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1113.201172\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1174.724609\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1215.837891\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1243.621094\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1307\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"1370.476562\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1402.263672\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1463.445312\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"1522.625\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1584.148438\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1625.261719\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1657.048828\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1696.257812\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1724.041016\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"1821.453125\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1882.976562\" xlink:href=\"#DejaVuSans-44\"/>\n     <use x=\"1914.763672\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1946.550781\" xlink:href=\"#DejaVuSans-75\"/>\n     <use x=\"2012.126953\" xlink:href=\"#DejaVuSans-61\"/>\n     <use x=\"2095.916016\" xlink:href=\"#DejaVuSans-51\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p85d1cf45af\">\n   <rect height=\"191.44\" width=\"381.1775\" x=\"84.585\" y=\"27.789\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "k = 3\n",
    "heterogeneity = []\n",
    "initial_centroids = get_initial_centroids(tf_idf, k, seed=0)\n",
    "centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,record_heterogeneity=heterogeneity, verbose=True)\n",
    "plot_heterogeneity(heterogeneity, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. (True/False) The clustering objective (heterogeneity) is non-increasing for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Let's step back from this particular example. If the clustering objective (heterogeneity) would ever increase when running k-means, that would indicate: (choose one)\n",
    "\n",
    "1. k-means algorithm got stuck in a bad local minimum\n",
    "2. There is a bug in the k-means code\n",
    "3. All data points consist of exact duplicates\n",
    "4. Nothing is wrong. The objective should generally go down sooner or later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Which of the cluster contains the greatest number of data points in the end? Hint: Use [`np.bincount()`](http://docs.scipy.org/doc/numpy-1.11.0/reference/generated/numpy.bincount.html) to count occurrences of each cluster label.\n",
    " 1. Cluster #0\n",
    " 2. Cluster #1\n",
    " 3. Cluster #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([19595, 10427, 29049])"
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "np.bincount(cluster_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beware of local maxima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One weakness of k-means is that it tends to get stuck in a local minimum. To see this, let us run k-means multiple times, with different initial centroids created using different random seeds.\n",
    "\n",
    "**Note:** Again, in practice, you should set different seeds for every run. We give you a list of seeds for this assignment so that everyone gets the same answer.\n",
    "\n",
    "This may take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "seed=000000, heterogeneity=57457.52442, cluster_distribution=[18047  3824  5671  6983  1492  1730  3882  3449  7139  6854]\nseed=020000, heterogeneity=57533.20100, cluster_distribution=[ 3142   768  3566  2277 15779  7278  6146  7964  6666  5485]\nseed=040000, heterogeneity=57512.69257, cluster_distribution=[ 5551  6623   186  2999  8487  3893  6807  2921  3472 18132]\nseed=060000, heterogeneity=57466.97925, cluster_distribution=[ 3014  3089  6681  3856  8080  7222  3424   424  5381 17900]\nseed=080000, heterogeneity=57494.92990, cluster_distribution=[17582  1785  7215  3314  6285   809  5930  6791  5536  3824]\nseed=100000, heterogeneity=57484.42210, cluster_distribution=[ 6618  1337  6191  2890 16969  4983  5242  3892  5562  5387]\nseed=120000, heterogeneity=57554.62410, cluster_distribution=[ 6118  5841  4964  8423  4302  3183 16481  1608  5524  2627]\n372.098571062088\n"
    }
   ],
   "source": [
    "k = 10\n",
    "heterogeneity = {}\n",
    "cluster_assignment_dict = {}\n",
    "import time\n",
    "start = time.time()\n",
    "for seed in [0, 20000, 40000, 60000, 80000, 100000, 120000]:\n",
    "    initial_centroids = get_initial_centroids(tf_idf, k, seed)\n",
    "    centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n",
    "                                           record_heterogeneity=None, verbose=False)\n",
    "    # To save time, compute heterogeneity only once in the end\n",
    "    heterogeneity[seed] = compute_heterogeneity(tf_idf, k, centroids, cluster_assignment)\n",
    "\n",
    "    # This is the line we added for the next quiz question\n",
    "    cluster_assignment_dict[seed] = np.bincount(cluster_assignment)\n",
    "    \n",
    "#    print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))\n",
    "    # And this is the modified print statement\n",
    "    print('seed={0:06d}, heterogeneity={1:.5f}, cluster_distribution={2}'.format(seed, heterogeneity[seed], \n",
    "                                           cluster_assignment_dict[seed]))\n",
    "    sys.stdout.flush()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the variation in heterogeneity for different initializations. This indicates that k-means sometimes gets stuck at a bad local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Another way to capture the effect of changing initialization is to look at the distribution of cluster assignments. Add a line to the code above to compute the size (# of member data points) of clusters for each run of k-means. Look at the size of the largest cluster (most # of member data points) across multiple runs, with seeds 0, 20000, ..., 120000. How much does this measure vary across the runs? What is the minimum and maximum values this quantity takes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One effective way to counter this tendency is to use **k-means++** to provide a smart initialization. This method tries to spread out the initial set of centroids so that they are not too close together. It is known to improve the quality of local optima and lower average runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_initialize(data, k, seed=None):\n",
    "    '''Use k-means++ to initialize a good set of centroids'''\n",
    "    if seed is not None: # useful for obtaining consistent results\n",
    "        np.random.seed(seed)\n",
    "    centroids = np.zeros((k, data.shape[1]))\n",
    "    \n",
    "    # Randomly choose the first centroid.\n",
    "    # Since we have no prior knowledge, choose uniformly at random\n",
    "    idx = np.random.randint(data.shape[0])\n",
    "    centroids[0] = data[idx,:].toarray()\n",
    "    # Compute distances from the first centroid chosen to all the other data points\n",
    "    squared_distances = pairwise_distances(data, centroids[0:1], metric='euclidean').flatten()**2\n",
    "    \n",
    "    for i in range(1, k):\n",
    "        # Choose the next centroid randomly, so that the probability for each data point to be chosen\n",
    "        # is directly proportional to its squared distance from the nearest centroid.\n",
    "        # Roughtly speaking, a new centroid should be as far as from ohter centroids as possible.\n",
    "        idx = np.random.choice(data.shape[0], 1, p=squared_distances/sum(squared_distances))\n",
    "        centroids[i] = data[idx,:].toarray()\n",
    "        # Now compute distances from the centroids to all data points\n",
    "        squared_distances = np.min(pairwise_distances(data, centroids[0:i+1], metric='euclidean')**2,axis=1)\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now rerun k-means with 10 clusters using the same set of seeds, but always using k-means++ to initialize the algorithm.\n",
    "\n",
    "This may take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "seed=000000, heterogeneity=57468.63808\nseed=020000, heterogeneity=57486.94263\nseed=040000, heterogeneity=57454.35926\nseed=060000, heterogeneity=57530.43659\nseed=080000, heterogeneity=57454.51852\nseed=100000, heterogeneity=57471.56674\nseed=120000, heterogeneity=57523.28839\nCPU times: user 8min 41s, sys: 16.8 s, total: 8min 58s\nWall time: 8min 56s\n"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "k = 10\n",
    "heterogeneity_smart = {}\n",
    "seeds = [0, 20000, 40000, 60000, 80000, 100000, 120000]\n",
    "for seed in seeds:\n",
    "    initial_centroids = smart_initialize(tf_idf, k, seed)\n",
    "    centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n",
    "                                           record_heterogeneity=None, verbose=False)\n",
    "    # To save time, compute heterogeneity only once in the end\n",
    "    heterogeneity_smart[seed] = compute_heterogeneity(tf_idf, k, centroids, cluster_assignment)\n",
    "    print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity_smart[seed]))\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the set of cluster heterogeneities we got from our 7 restarts of k-means using random initialization compared to the 7 restarts of k-means using k-means++ as a smart initialization.\n",
    "\n",
    "The following code produces a [box plot](http://matplotlib.org/api/pyplot_api.html) for each of these methods, indicating the spread of values produced by each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 576x360 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"340.325pt\" version=\"1.1\" viewBox=\"0 0 555.4025 340.325\" width=\"555.4025pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 340.325 \nL 555.4025 340.325 \nL 555.4025 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 109.7625 310.64 \nL 548.2025 310.64 \nL 548.2025 7.2 \nL 109.7625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m9629e769c1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"152.115167\" xlink:href=\"#m9629e769c1\" y=\"310.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 57460 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(126.665167 329.7975)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"231.620968\" xlink:href=\"#m9629e769c1\" y=\"310.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 57480 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(206.170968 329.7975)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"311.126769\" xlink:href=\"#m9629e769c1\" y=\"310.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 57500 -->\n      <g transform=\"translate(285.676769 329.7975)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"390.63257\" xlink:href=\"#m9629e769c1\" y=\"310.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 57520 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(365.18257 329.7975)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.13837\" xlink:href=\"#m9629e769c1\" y=\"310.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 57540 -->\n      <g transform=\"translate(444.68837 329.7975)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"maaec943119\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"109.7625\" xlink:href=\"#maaec943119\" y=\"234.78\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- k-means -->\n      <defs>\n       <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n       <path d=\"M 4.890625 31.390625 \nL 31.203125 31.390625 \nL 31.203125 23.390625 \nL 4.890625 23.390625 \nz\n\" id=\"DejaVuSans-45\"/>\n       <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <g transform=\"translate(34.015 240.85875)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"57.910156\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"93.994141\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"191.40625\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"252.929688\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"314.208984\" xlink:href=\"#DejaVuSans-110\"/>\n       <use x=\"377.587891\" xlink:href=\"#DejaVuSans-115\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"109.7625\" xlink:href=\"#maaec943119\" y=\"83.06\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- k-means++ -->\n      <defs>\n       <path d=\"M 46 62.703125 \nL 46 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 46 27.203125 \nL 46 0 \nL 37.796875 0 \nL 37.796875 27.203125 \nL 10.59375 27.203125 \nL 10.59375 35.5 \nL 37.796875 35.5 \nL 37.796875 62.703125 \nz\n\" id=\"DejaVuSans-43\"/>\n      </defs>\n      <g transform=\"translate(7.2 89.13875)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"57.910156\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"93.994141\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"191.40625\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"252.929688\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"314.208984\" xlink:href=\"#DejaVuSans-110\"/>\n       <use x=\"377.587891\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"429.6875\" xlink:href=\"#DejaVuSans-43\"/>\n       <use x=\"513.476562\" xlink:href=\"#DejaVuSans-43\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_8\">\n    <path clip-path=\"url(#p25c3dc6320)\" d=\"M 214.529889 246.159 \nL 214.529889 223.401 \nL 402.346893 223.401 \nL 402.346893 246.159 \nL 214.529889 246.159 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"line2d_9\">\n    <path clip-path=\"url(#p25c3dc6320)\" d=\"M 214.529889 234.78 \nL 142.27403 234.78 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"line2d_10\">\n    <path clip-path=\"url(#p25c3dc6320)\" d=\"M 402.346893 234.78 \nL 528.273409 234.78 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"line2d_11\">\n    <path clip-path=\"url(#p25c3dc6320)\" d=\"M 142.27403 240.4695 \nL 142.27403 229.0905 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"line2d_12\">\n    <path clip-path=\"url(#p25c3dc6320)\" d=\"M 528.273409 240.4695 \nL 528.273409 229.0905 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"line2d_13\"/>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p25c3dc6320)\" d=\"M 158.389372 94.439 \nL 158.389372 71.681 \nL 331.462417 71.681 \nL 331.462417 94.439 \nL 158.389372 94.439 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p25c3dc6320)\" d=\"M 158.389372 83.06 \nL 129.691591 83.06 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p25c3dc6320)\" d=\"M 331.462417 83.06 \nL 432.121022 83.06 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p25c3dc6320)\" d=\"M 129.691591 88.7495 \nL 129.691591 77.3705 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p25c3dc6320)\" d=\"M 432.121022 88.7495 \nL 432.121022 77.3705 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"line2d_19\"/>\n   <g id=\"line2d_20\">\n    <path clip-path=\"url(#p25c3dc6320)\" d=\"M 290.971639 246.159 \nL 290.971639 223.401 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path clip-path=\"url(#p25c3dc6320)\" d=\"M 198.096298 94.439 \nL 198.096298 71.681 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 109.7625 310.64 \nL 109.7625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 548.2025 310.64 \nL 548.2025 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 109.7625 310.64 \nL 548.2025 310.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 109.7625 7.2 \nL 548.2025 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p25c3dc6320\">\n   <rect height=\"303.44\" width=\"438.44\" x=\"109.7625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.boxplot([list(heterogeneity.values()), list(heterogeneity_smart.values())], vert=False)\n",
    "plt.yticks([1, 2], ['k-means', 'k-means++'])\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to notice from the box plot:\n",
    "* On average, k-means++ produces a better clustering than Random initialization.\n",
    "* Variation in clustering quality is smaller for k-means++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In general, you should run k-means at least a few times with different initializations and then return the run resulting in the lowest heterogeneity.** Let us write a function that runs k-means multiple times and picks the best run that minimizes heterogeneity. The function accepts an optional list of seed values to be used for the multiple runs; if no such list is provided, the current UTC time is used as seed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_multiple_runs(data, k, maxiter, num_runs, seed_list=None, verbose=False):\n",
    "    heterogeneity = {}\n",
    "    \n",
    "    min_heterogeneity_achieved = float('inf')\n",
    "    best_seed = None\n",
    "    final_centroids = None\n",
    "    final_cluster_assignment = None\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        \n",
    "        # Use UTC time if no seeds are provided \n",
    "        if seed_list is not None: \n",
    "            seed = seed_list[i]\n",
    "            np.random.seed(seed)\n",
    "        else: \n",
    "            seed = int(time.time())\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Use k-means++ initialization\n",
    "        # YOUR CODE HERE\n",
    "        initial_centroids = smart_initialize(data,k,seed)\n",
    "        \n",
    "        # Run k-means\n",
    "        # YOUR CODE HERE\n",
    "        centroids, cluster_assignment = kmeans(data,k,initial_centroids,maxiter,None,verbose)\n",
    "        \n",
    "        # To save time, compute heterogeneity only once in the end\n",
    "        # YOUR CODE HERE\n",
    "        heterogeneity[seed] = compute_heterogeneity(data,k,centroids,cluster_assignment)\n",
    "        \n",
    "        if verbose:\n",
    "            print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        # if current measurement of heterogeneity is lower than previously seen,\n",
    "        # update the minimum record of heterogeneity.\n",
    "        if heterogeneity[seed] < min_heterogeneity_achieved:\n",
    "            min_heterogeneity_achieved = heterogeneity[seed]\n",
    "            best_seed = seed\n",
    "            final_centroids = centroids\n",
    "            final_cluster_assignment = cluster_assignment\n",
    "    \n",
    "    # Return the centroids and cluster assignments that minimize heterogeneity.\n",
    "    return final_centroids, final_cluster_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are measuring the tightness of the clusters, a higher value of K reduces the possible heterogeneity metric by definition.  For example, if we have N data points and set K=N clusters, then we could have 0 cluster heterogeneity by setting the N centroids equal to the values of the N data points. (Note: Not all runs for larger K will result in lower heterogeneity than a single run with smaller K due to local optima.)  Let's explore this general trend for ourselves by performing the following analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `kmeans_multiple_runs` function to run k-means with five different values of K.  For each K, use k-means++ and multiple runs to pick the best solution.  In what follows, we consider K=2,10,25,50,100 and 7 restarts for each setting.\n",
    "\n",
    "**IMPORTANT: The code block below will take about 10 minutes to finish**\n",
    "\n",
    "In order to speed up the computations, we run them with only one random seed, but for better performance, one should use more seeds and compare the results. If you don't mind running the code for approximately one hour, feel free to uncomment the following line of code below:\n",
    "\n",
    "`seed_list = [0]#, 20000, 40000, 60000, 80000, 100000, 120000]`\n",
    "\n",
    "Side note: In practice, a good implementation of k-means would utilize parallelism to run multiple runs of k-means at once. For an example, see [scikit-learn's KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false,
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "gnment.\n57\n       10 elements changed their cluster assignment.\n58\n       11 elements changed their cluster assignment.\n59\n        9 elements changed their cluster assignment.\n60\n        6 elements changed their cluster assignment.\n61\n        9 elements changed their cluster assignment.\n62\n        9 elements changed their cluster assignment.\n63\n        7 elements changed their cluster assignment.\n64\n        3 elements changed their cluster assignment.\n65\n        2 elements changed their cluster assignment.\n66\n        2 elements changed their cluster assignment.\n67\n        3 elements changed their cluster assignment.\n68\n        1 elements changed their cluster assignment.\n69\nseed=000000, heterogeneity=55649.66538\n0\n1\n    23878 elements changed their cluster assignment.\n2\n    11682 elements changed their cluster assignment.\n3\n     7273 elements changed their cluster assignment.\n4\n     4989 elements changed their cluster assignment.\n5\n     3246 elements changed their cluster assignment.\n6\n     2125 elements changed their cluster assignment.\n7\n     1615 elements changed their cluster assignment.\n8\n     1196 elements changed their cluster assignment.\n9\n      926 elements changed their cluster assignment.\n10\n      733 elements changed their cluster assignment.\n11\n      584 elements changed their cluster assignment.\n12\n      475 elements changed their cluster assignment.\n13\n      414 elements changed their cluster assignment.\n14\n      315 elements changed their cluster assignment.\n15\n      256 elements changed their cluster assignment.\n16\n      221 elements changed their cluster assignment.\n17\n      199 elements changed their cluster assignment.\n18\n      167 elements changed their cluster assignment.\n19\n      129 elements changed their cluster assignment.\n20\n      101 elements changed their cluster assignment.\n21\n       80 elements changed their cluster assignment.\n22\n       68 elements changed their cluster assignment.\n23\n       54 elements changed their cluster assignment.\n24\n       53 elements changed their cluster assignment.\n25\n       52 elements changed their cluster assignment.\n26\n       63 elements changed their cluster assignment.\n27\n       83 elements changed their cluster assignment.\n28\n      104 elements changed their cluster assignment.\n29\n      148 elements changed their cluster assignment.\n30\n      159 elements changed their cluster assignment.\n31\n      171 elements changed their cluster assignment.\n32\n      116 elements changed their cluster assignment.\n33\n       80 elements changed their cluster assignment.\n34\n       61 elements changed their cluster assignment.\n35\n       50 elements changed their cluster assignment.\n36\n       69 elements changed their cluster assignment.\n37\n       63 elements changed their cluster assignment.\n38\n       51 elements changed their cluster assignment.\n39\n       40 elements changed their cluster assignment.\n40\n       31 elements changed their cluster assignment.\n41\n       23 elements changed their cluster assignment.\n42\n       42 elements changed their cluster assignment.\n43\n       36 elements changed their cluster assignment.\n44\n       27 elements changed their cluster assignment.\n45\n       15 elements changed their cluster assignment.\n46\n       13 elements changed their cluster assignment.\n47\n       13 elements changed their cluster assignment.\n48\n       17 elements changed their cluster assignment.\n49\n       12 elements changed their cluster assignment.\n50\n       16 elements changed their cluster assignment.\n51\n        9 elements changed their cluster assignment.\n52\n        9 elements changed their cluster assignment.\n53\n       11 elements changed their cluster assignment.\n54\n        9 elements changed their cluster assignment.\n55\n        6 elements changed their cluster assignment.\n56\n        6 elements changed their cluster assignment.\n57\n        4 elements changed their cluster assignment.\n58\n        5 elements changed their cluster assignment.\n59\n        8 elements changed their cluster assignment.\n60\n       10 elements changed their cluster assignment.\n61\n       11 elements changed their cluster assignment.\n62\n       18 elements changed their cluster assignment.\n63\n       27 elements changed their cluster assignment.\n64\n       31 elements changed their cluster assignment.\n65\n       26 elements changed their cluster assignment.\n66\n       19 elements changed their cluster assignment.\n67\n        9 elements changed their cluster assignment.\n68\n        8 elements changed their cluster assignment.\n69\n        7 elements changed their cluster assignment.\n70\n        7 elements changed their cluster assignment.\n71\n        3 elements changed their cluster assignment.\n72\n        3 elements changed their cluster assignment.\n73\n        1 elements changed their cluster assignment.\n74\nseed=020000, heterogeneity=55587.56988\n0\n1\n    23726 elements changed their cluster assignment.\n2\n    10544 elements changed their cluster assignment.\n3\n     6365 elements changed their cluster assignment.\n4\n     3897 elements changed their cluster assignment.\n5\n     2634 elements changed their cluster assignment.\n6\n     1859 elements changed their cluster assignment.\n7\n     1467 elements changed their cluster assignment.\n8\n     1140 elements changed their cluster assignment.\n9\n      956 elements changed their cluster assignment.\n10\n      852 elements changed their cluster assignment.\n11\n      793 elements changed their cluster assignment.\n12\n      712 elements changed their cluster assignment.\n13\n      608 elements changed their cluster assignment.\n14\n      529 elements changed their cluster assignment.\n15\n      424 elements changed their cluster assignment.\n16\n      319 elements changed their cluster assignment.\n17\n      243 elements changed their cluster assignment.\n18\n      172 elements changed their cluster assignment.\n19\n      133 elements changed their cluster assignment.\n20\n      117 elements changed their cluster assignment.\n21\n      112 elements changed their cluster assignment.\n22\n      105 elements changed their cluster assignment.\n23\n       89 elements changed their cluster assignment.\n24\n       84 elements changed their cluster assignment.\n25\n       77 elements changed their cluster assignment.\n26\n       77 elements changed their cluster assignment.\n27\n       77 elements changed their cluster assignment.\n28\n       79 elements changed their cluster assignment.\n29\n       91 elements changed their cluster assignment.\n30\n       96 elements changed their cluster assignment.\n31\n       96 elements changed their cluster assignment.\n32\n      112 elements changed their cluster assignment.\n33\n      115 elements changed their cluster assignment.\n34\n      147 elements changed their cluster assignment.\n35\n      220 elements changed their cluster assignment.\n36\n      285 elements changed their cluster assignment.\n37\n      243 elements changed their cluster assignment.\n38\n      217 elements changed their cluster assignment.\n39\n      195 elements changed their cluster assignment.\n40\n      145 elements changed their cluster assignment.\n41\n      117 elements changed their cluster assignment.\n42\n       97 elements changed their cluster assignment.\n43\n       75 elements changed their cluster assignment.\n44\n       60 elements changed their cluster assignment.\n45\n       49 elements changed their cluster assignment.\n46\n       33 elements changed their cluster assignment.\n47\n       21 elements changed their cluster assignment.\n48\n       21 elements changed their cluster assignment.\n49\n       14 elements changed their cluster assignment.\n50\n       16 elements changed their cluster assignment.\n51\n       11 elements changed their cluster assignment.\n52\n       12 elements changed their cluster assignment.\n53\n       11 elements changed their cluster assignment.\n54\n       13 elements changed their cluster assignment.\n55\n       15 elements changed their cluster assignment.\n56\n       14 elements changed their cluster assignment.\n57\n       17 elements changed their cluster assignment.\n58\n       20 elements changed their cluster assignment.\n59\n       27 elements changed their cluster assignment.\n60\n       28 elements changed their cluster assignment.\n61\n       23 elements changed their cluster assignment.\n62\n       29 elements changed their cluster assignment.\n63\n       30 elements changed their cluster assignment.\n64\n       38 elements changed their cluster assignment.\n65\n       48 elements changed their cluster assignment.\n66\n       75 elements changed their cluster assignment.\n67\n       91 elements changed their cluster assignment.\n68\n       81 elements changed their cluster assignment.\n69\n       25 elements changed their cluster assignment.\n70\n        7 elements changed their cluster assignment.\n71\n        1 elements changed their cluster assignment.\n72\nseed=040000, heterogeneity=55720.24668\n0\n1\n    23785 elements changed their cluster assignment.\n2\n    10084 elements changed their cluster assignment.\n3\n     6469 elements changed their cluster assignment.\n4\n     4645 elements changed their cluster assignment.\n5\n     3337 elements changed their cluster assignment.\n6\n     2618 elements changed their cluster assignment.\n7\n     2043 elements changed their cluster assignment.\n8\n     1637 elements changed their cluster assignment.\n9\n     1253 elements changed their cluster assignment.\n10\n      963 elements changed their cluster assignment.\n11\n      728 elements changed their cluster assignment.\n12\n      618 elements changed their cluster assignment.\n13\n      526 elements changed their cluster assignment.\n14\n      491 elements changed their cluster assignment.\n15\n      432 elements changed their cluster assignment.\n16\n      391 elements changed their cluster assignment.\n17\n      356 elements changed their cluster assignment.\n18\n      328 elements changed their cluster assignment.\n19\n      335 elements changed their cluster assignment.\n20\n      340 elements changed their cluster assignment.\n21\n      325 elements changed their cluster assignment.\n22\n      310 elements changed their cluster assignment.\n23\n      275 elements changed their cluster assignment.\n24\n      207 elements changed their cluster assignment.\n25\n      162 elements changed their cluster assignment.\n26\n      119 elements changed their cluster assignment.\n27\n      108 elements changed their cluster assignment.\n28\n       95 elements changed their cluster assignment.\n29\n       92 elements changed their cluster assignment.\n30\n       90 elements changed their cluster assignment.\n31\n       88 elements changed their cluster assignment.\n32\n       81 elements changed their cluster assignment.\n33\n       82 elements changed their cluster assignment.\n34\n       92 elements changed their cluster assignment.\n35\n      107 elements changed their cluster assignment.\n36\n      156 elements changed their cluster assignment.\n37\n      191 elements changed their cluster assignment.\n38\n      185 elements changed their cluster assignment.\n39\n      155 elements changed their cluster assignment.\n40\n      117 elements changed their cluster assignment.\n41\n       71 elements changed their cluster assignment.\n42\n       46 elements changed their cluster assignment.\n43\n       36 elements changed their cluster assignment.\n44\n       26 elements changed their cluster assignment.\n45\n       17 elements changed their cluster assignment.\n46\n       15 elements changed their cluster assignment.\n47\n        8 elements changed their cluster assignment.\n48\n        6 elements changed their cluster assignment.\n49\n        7 elements changed their cluster assignment.\n50\n        4 elements changed their cluster assignment.\n51\n        4 elements changed their cluster assignment.\n52\n        7 elements changed their cluster assignment.\n53\n        8 elements changed their cluster assignment.\n54\n        2 elements changed their cluster assignment.\n55\n        2 elements changed their cluster assignment.\n56\n        2 elements changed their cluster assignment.\n57\n        1 elements changed their cluster assignment.\n58\n        2 elements changed their cluster assignment.\n59\n        1 elements changed their cluster assignment.\n60\n        2 elements changed their cluster assignment.\n61\nseed=060000, heterogeneity=55616.64653\n0\n1\n    22878 elements changed their cluster assignment.\n2\n    10389 elements changed their cluster assignment.\n3\n     5827 elements changed their cluster assignment.\n4\n     3850 elements changed their cluster assignment.\n5\n     2694 elements changed their cluster assignment.\n6\n     2176 elements changed their cluster assignment.\n7\n     1809 elements changed their cluster assignment.\n8\n     1442 elements changed their cluster assignment.\n9\n     1083 elements changed their cluster assignment.\n10\n      780 elements changed their cluster assignment.\n11\n      579 elements changed their cluster assignment.\n12\n      394 elements changed their cluster assignment.\n13\n      320 elements changed their cluster assignment.\n14\n      320 elements changed their cluster assignment.\n15\n      372 elements changed their cluster assignment.\n16\n      328 elements changed their cluster assignment.\n17\n      345 elements changed their cluster assignment.\n18\n      411 elements changed their cluster assignment.\n19\n      389 elements changed their cluster assignment.\n20\n      346 elements changed their cluster assignment.\n21\n      246 elements changed their cluster assignment.\n22\n      211 elements changed their cluster assignment.\n23\n      172 elements changed their cluster assignment.\n24\n      138 elements changed their cluster assignment.\n25\n      125 elements changed their cluster assignment.\n26\n       96 elements changed their cluster assignment.\n27\n       95 elements changed their cluster assignment.\n28\n      102 elements changed their cluster assignment.\n29\n       89 elements changed their cluster assignment.\n30\n       99 elements changed their cluster assignment.\n31\n      124 elements changed their cluster assignment.\n32\n      123 elements changed their cluster assignment.\n33\n      114 elements changed their cluster assignment.\n34\n      103 elements changed their cluster assignment.\n35\n      107 elements changed their cluster assignment.\n36\n      104 elements changed their cluster assignment.\n37\n       99 elements changed their cluster assignment.\n38\n       75 elements changed their cluster assignment.\n39\n       66 elements changed their cluster assignment.\n40\n       60 elements changed their cluster assignment.\n41\n       35 elements changed their cluster assignment.\n42\n       21 elements changed their cluster assignment.\n43\n       23 elements changed their cluster assignment.\n44\n       13 elements changed their cluster assignment.\n45\n       13 elements changed their cluster assignment.\n46\n        9 elements changed their cluster assignment.\n47\n        2 elements changed their cluster assignment.\n48\n        3 elements changed their cluster assignment.\n49\n        7 elements changed their cluster assignment.\n50\n       10 elements changed their cluster assignment.\n51\n        7 elements changed their cluster assignment.\n52\n        7 elements changed their cluster assignment.\n53\n        4 elements changed their cluster assignment.\n54\n        7 elements changed their cluster assignment.\n55\n        7 elements changed their cluster assignment.\n56\n        3 elements changed their cluster assignment.\n57\n        4 elements changed their cluster assignment.\n58\n        2 elements changed their cluster assignment.\n59\nseed=080000, heterogeneity=55672.95812\n0\n1\n    23498 elements changed their cluster assignment.\n2\n    12068 elements changed their cluster assignment.\n3\n     7039 elements changed their cluster assignment.\n4\n     4491 elements changed their cluster assignment.\n5\n     3261 elements changed their cluster assignment.\n6\n     2290 elements changed their cluster assignment.\n7\n     1633 elements changed their cluster assignment.\n8\n     1485 elements changed their cluster assignment.\n9\n     1384 elements changed their cluster assignment.\n10\n     1210 elements changed their cluster assignment.\n11\n      952 elements changed their cluster assignment.\n12\n      739 elements changed their cluster assignment.\n13\n      607 elements changed their cluster assignment.\n14\n      548 elements changed their cluster assignment.\n15\n      505 elements changed their cluster assignment.\n16\n      386 elements changed their cluster assignment.\n17\n      290 elements changed their cluster assignment.\n18\n      219 elements changed their cluster assignment.\n19\n      148 elements changed their cluster assignment.\n20\n      107 elements changed their cluster assignment.\n21\n      100 elements changed their cluster assignment.\n22\n       79 elements changed their cluster assignment.\n23\n       70 elements changed their cluster assignment.\n24\n       71 elements changed their cluster assignment.\n25\n       54 elements changed their cluster assignment.\n26\n       48 elements changed their cluster assignment.\n27\n       29 elements changed their cluster assignment.\n28\n       16 elements changed their cluster assignment.\n29\n        8 elements changed their cluster assignment.\n30\n        1 elements changed their cluster assignment.\n31\n        2 elements changed their cluster assignment.\n32\nseed=100000, heterogeneity=55660.45384\n0\n1\n    22713 elements changed their cluster assignment.\n2\n    10982 elements changed their cluster assignment.\n3\n     6886 elements changed their cluster assignment.\n4\n     3962 elements changed their cluster assignment.\n5\n     2650 elements changed their cluster assignment.\n6\n     1859 elements changed their cluster assignment.\n7\n     1501 elements changed their cluster assignment.\n8\n     1383 elements changed their cluster assignment.\n9\n     1109 elements changed their cluster assignment.\n10\n      984 elements changed their cluster assignment.\n11\n      905 elements changed their cluster assignment.\n12\n      861 elements changed their cluster assignment.\n13\n      851 elements changed their cluster assignment.\n14\n      881 elements changed their cluster assignment.\n15\n      639 elements changed their cluster assignment.\n16\n      479 elements changed their cluster assignment.\n17\n      375 elements changed their cluster assignment.\n18\n      246 elements changed their cluster assignment.\n19\n      221 elements changed their cluster assignment.\n20\n      193 elements changed their cluster assignment.\n21\n      168 elements changed their cluster assignment.\n22\n      134 elements changed their cluster assignment.\n23\n      105 elements changed their cluster assignment.\n24\n       97 elements changed their cluster assignment.\n25\n      127 elements changed their cluster assignment.\n26\n      145 elements changed their cluster assignment.\n27\n      145 elements changed their cluster assignment.\n28\n      132 elements changed their cluster assignment.\n29\n      117 elements changed their cluster assignment.\n30\n      110 elements changed their cluster assignment.\n31\n      124 elements changed their cluster assignment.\n32\n      145 elements changed their cluster assignment.\n33\n      163 elements changed their cluster assignment.\n34\n      150 elements changed their cluster assignment.\n35\n      157 elements changed their cluster assignment.\n36\n      141 elements changed their cluster assignment.\n37\n      125 elements changed their cluster assignment.\n38\n       87 elements changed their cluster assignment.\n39\n       54 elements changed their cluster assignment.\n40\n       37 elements changed their cluster assignment.\n41\n       24 elements changed their cluster assignment.\n42\n       11 elements changed their cluster assignment.\n43\n        7 elements changed their cluster assignment.\n44\n        9 elements changed their cluster assignment.\n45\n        4 elements changed their cluster assignment.\n46\n        3 elements changed their cluster assignment.\n47\nseed=120000, heterogeneity=55735.28103\nCPU times: user 2h 15s, sys: 11min 27s, total: 2h 11min 43s\nWall time: 2h 12min 53s\n"
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np \n",
    "\n",
    "def plot_k_vs_heterogeneity(k_values, heterogeneity_values):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(k_values, heterogeneity_values, linewidth=4)\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Heterogeneity')\n",
    "    plt.title('K vs. Heterogeneity')\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()\n",
    "\n",
    "centroids = {}\n",
    "cluster_assignment = {}\n",
    "heterogeneity_values = []\n",
    "k_list = [2, 10, 25, 50, 100]\n",
    "seed_list = [0]\n",
    "# Uncomment the following line to run the plot with all the seeds (it may take about an hour to finish).\n",
    "seed_list = [0, 20000, 40000, 60000, 80000, 100000, 120000]\n",
    "\n",
    "for k in k_list:\n",
    "    heterogeneity = []\n",
    "    centroids[k], cluster_assignment[k] = kmeans_multiple_runs(tf_idf, k, maxiter=400,\n",
    "                                                               num_runs=len(seed_list),                                                               seed_list=seed_list,\n",
    "                                                               verbose=True)\n",
    "    score = compute_heterogeneity(tf_idf, k, centroids[k], cluster_assignment[k])\n",
    "    heterogeneity_values.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAELCAYAAADqYO7XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3yV5d3H8c8vCWGvhI1AAAeCiCgq4J44sda6tY6q1VpHn/Z5qm0ddbT2sX2cVWutgnvXUfdeDGUqCCqEsBFI2BDI+D1/3HfiyeFkHHJyTpLzfb9e53XIdV3nPr9zk+SX+7qvYe6OiIiIpE5GqgMQERFJd0rGIiIiKaZkLCIikmJKxiIiIimmZCwiIpJiSsYiIiIppmQsIpIiZnaombmZ3ZjqWCS1lIwlbZhZXviL76UYdWZmd4T1U8wsNxUxxsvMCsxsbQ31ncLP9GEC3qegPseQukvE/5k0LVmpDkAk1cwsA3gQ+BnwMXCiu69PbVSSJj4HdgdWpzoQSS0lY0lrZtYCeBw4DXgDOMXdt6Q2KkkX7r4ZmJvqOCT11E0tacvMWgMvESTi54CT6pKIzeyRsBtxRDX1D4b1oyLKTjOzT81stZltMbOFZvaSmR2YqM8TLzPrYGa3mNlcMys2s8IwpmERbfLMzIF+QL/wc1U8zo9ol2FmF5vZZDPbGD4mmNmPY7zvuPD1A83st2b2rZlti7xvGr7vODNbHtYtNLO7zaxLNZ/llxGfI9/MrguP72Y2Lkb7geH/45Lw+EvM7D4z6xrVruLWxjgz2zk8P+vMbIOZvWpmA6uJZ7iZPWdm35vZVjObb2Z/NrN2Ue2q3DOu+DqsPiTqfB9qZteH//5RNe97U1h/cqx6abx0ZSxpyczaA68ChwAPAxe7e3kdX/44cD5wNjAl6rjZwE+AfHefGJZdDtwLzAeeBjYCvcL3PhT4tH6fJn5hUvuYoIv0A+A1IBc4BTjKzI4M418L/BG4OnzpnRGHmREey4CnCP6o+RoYH9YfD7xgZle7+10xwrgX2Cd875eB/PB4uxGck1yCP5a+AfYGrgCON7OR7r4q4rP8CbgWWAzcD2QCvwD2r+azjwLeBFoBrwALgEHApcAYM9vX3YuiXpYHTAJmAv8EhgInAEPMbEjkH3FhInwa2BbGvyKM/xrgMDM72N23xYoNKCA43zcAC4FxUXXfAdcT3FKpMvYhvN1yHrAS+E81x5fGyt310CMtHgS/UB34iOBenQN3ABbncTKApcByICOq7qTwuDdFlE0DlgBtotoakFPPz1QAFAM3VvO4LYznw6jXPRWWnxlVvjOwDvgqxvsUVBPDz8Nj/R3IjChvC0wGtgK9IsrHhe0XRJZH1H8Q1p8bVX59WP5wRNkgoIwgSXWMKO8W/h85MC6iPJsgyRUBu0cd/9Sw/b0xvmcc+HVU+0eizyHQBVhP8IdFr6j2/x22/01E2aFh2Y1Rbbf7P4uoexUoBXpGlY8JX3d7qn/W9Ij/kfIA9NAjWY+oX6wOvF2PY/0tPMaRUeXPhuW7RZRNC385ZzfAZyqI+kzVPT6MeE2XMIH9p5pj/jV8zR5R71NQTfsvw+S23ecjuHp04JcRZRXJ+PIY7fuGddNi1LUiuMrcUvFeBH9wOHBJjPb/EyMZ/zg6IUa9ZgqwOsb3zHy2/8PrkLDubxFl/xWW/STGsTMIrlqnRJTtSDIeG9ZfE1X+TFg+KBk/T3ok9qFuaklHXwI9CLpjr/LYXai1eZzgF+/ZwLsQ3IMFTiT4ZftNRNtnCK5QZ5nZMwRX5hPdfVM9PkOkde7eKVaFmXUC1kQV70uQGNpZ7Pmtu4fPg4BZNb2xmbUB9gAWAb8LeqyrqLgHOyjGy6fEKNsrfP4wusLdi81sEkHvw27AV0DF/e0JMY41MUZZRdf1HtV89tZArpl1cffIEc5f+va3MZaGz5HnvuL4B5rZHjGOX0LscxGP14BlwIUE31dYMBXvJGCCu2tAWBOkZCzpaAFwFvA+cKeZubvfHc8B3H26mc0Bfmxml7l7McFVVyuCRB3pfwkS4mXAH8JHsZk9DfyXu0cny4aWEz4fEj6q07YOx+pM0N3ej+A+ZzzHWhmjrEP4/H01x1kR1a59+LwqRttYx6/47OdVc/wKbak63WhdjDal4XNmjONfVcvxd5i7l5nZI8Dvw/vPHwPnAC2BhxrqfaVhaTS1pCV3nw0cTvBL/C4zu2IHDvMEQVI4Ifz6bILu36ej3svd/UF3H05wRX46wdXx+VQdoJMsFXOob3V3q+ExvsajVD3WZ7Uc64IYr/UYZRXH617N+3WParchfO4ao223Go5/VC3xLqzm/WtTcfxdajr+Dh470kME5+/C8OsLCQYGPpuAY0sKKBlL2goT8mEECfluM/tlnId4kuAX4llm1pMgub/r7tVd1eHu37v7s8BxBIOOjjWzZPdQfUEQ98g4XlNG1StAANx9A8E82T2ip+3soBnh88HRFWbWkqAbuJhghDUEo5sBRsc41qgYZZ+Hz/F89ngk6vjlxDjfFdy9gOD2yKlmdjiwJ/B0Am99SJIpGUtai7pCviechlTX1y4guFd5HMG0mAy276LGzI42s+hfrG2AdgTTX8oj2g4ys/reU6yRu68AngeOMLPLYsSbYWbR3ddFQJcwIUa7B+gI3GdmrWIcb4iZxbpKjRXbIoJeg33M7PSo6t8APQmSTsXUoGcIzt9/m1nHiPfsSuyu4pcIpkBdY2b7xYi1tZnFnBJVR48QXKH+xcx2iXH8TmY2vA7HKQJ619LmQYLvo8fCr/8VT6DSuOiesaQ9d58VXl28D9wb3kO+r44vfwI4gGCe62ai5n6GngU2mtmnBNNq2hDMwe0J3BI1MGhO+JyIrsyaXEYwkOg+M7uI4IpuI8Fo5lEEXbyRifUDYATwspl9RjAQ6TV3/4pgbu9o4FzgUDN7n+Debk+CK7a9wmPGuodbXWyfAk+a2anAtwTzdMcQ3O//bUVDd59jZn8lGDn9lZk9T3BFeRowlWA+d3lE+63hMd8AJpnZ2wRzo7MIRk4fQjDw65g6xlqFu680s7MJ/kiYbWavE/SAtAUGhMcfT/DHW00+ILjqfYZgwGEZ8GT4x0qFlwnOaS9gtrtP2pGYpZFI9XBuPfRI1oMfpqm8VE39UIJfbuXAZXU8Zi7B1a0DT1TT5jKCuaELCbpYVxIsuHF6jLYe/FjW+TMVAGtrqO9ENdNkCBLE74DpwCaCZPwdwRzkH0e17UBw5bWCIDE4cH5Um7MJksgagrnFi4C3ws/fNqLduPD1eTXEPQB4NHy/beGx7gW6xWhrwJUEXddbCRL2dQRd2g7cFeM1fflhIZatYcxfEVzl7xvje2ZcDd9PseoGh59zcRj/aoIpbrcRMfWI6qc29SLovSgMvx8dODTG+9wd1v0q1T9fetTvYeF/qIhIs2JmPyMY6HS5172no0kxs48J/ujo7VWnYkkTo3vGItKkmVnX6Hvy4YC63xNcVTbLpSHNbB/gIOAFJeKmT/eMRaSpOw+4MuJedR+Ce/IdCe7JL6rpxU2NmZ1FcL//fIK5zn9OaUCSEErGItLUfUowMGoMwaIb2wju/z7g7o+mMrAGcgnBFXE+8FMPBtFJE6d7xiIiIimmK+ME69Kli+fl5aU6DBERaYSmTp262t23WzFOyTjB8vLymDIl1vr3IiKS7sws5lKrGk0tIiKSYkrGIiIiKaZkLCIikmJKxiIiIimmZNwILS7azBOTd3Q7VRERaWo0mroRKS4p44GP5nP/h/PZVlbO0N4d2XOnTqkOS0REGpiujBuRnz82lTvf/Y6tpeW4w3Uvz6a8XIuyiIg0d0rGjcjPDuxf5euZi9fy7JTFKYpGRESSRcm4ETl4164cM6RHlbK/vDmXtZu3pSgiERFJBiXjRua6EwfTqsUP/y1rNpdw+1vfpDAiERFpaErGjUzvTq254vBdqpQ9+fkivlqyLkURiYhIQ1MyboQuOqg//bu0rfw6GMw1S4O5RESaKSXjRqhlViY3nDi4StmMxWt5fuqSFEUkIiINScm4kTp0t24cPbh7lbLb3pzLus0lKYpIREQaSlKTsZkdamYe47E2qt0QM3vRzJaZ2SYzm21mvzazrKh2GWZ2rZkVmFmxmc00s1Oqee+LzWyumW01s2/M7NJq2v3IzKaHx1toZn8ws8zEnYW6u+6EqoO5ijZt469vazCXiEhzk6or4yuBURGPIysqzKwX8CEwALgaOBF4CbgduDXqODcDNwL3AscCk4DnzOy4yEZmdjHwD+AF4BjgOeA+M7ssqt2YsM0X4fHuAv4A/Kl+H3fH9Mlpw+WH7lyl7InJC5m1VIO5RESaE3NP3qAgMzsU+AA4yt3frabNJQSJczd3/zai/GngEHfvGX7dDVgM3ObuN0S0ew/o6u57hl9nAcuAN9z9vIh2DwNjgZ7uXhKWTQfWu/shEe2uJ0jIfd19RW2fccSIET5lypS6nI46KS4pY8ydH7OwcHNl2d59O/H8paPJyLCEvY+IiDQ8M5vq7iOiyxvjPePs8Hl9VPlaqsY7Jmz7eFS7x4GhZlaxnNUooGuMdo8BucCBAGbWB9irmnYtCK6Uk65Vi0xuPHFIlbJpi9bywjQN5hIRaS5SlYyfMLMyMys0syfNrG9E3XPAauBeM+tvZh3M7GTgXOBvEe2GAFuBeVHHnh0+D45oBzBrR9q5+wJgc0S7pDtsUDeOih7M9YYGc4mINBfJTsbrCBLqRcDhBPd8jwQmht3OuPv3BFezuwP54WteAP7i7v8bcawcYK1v389eFFEf+bxmB9tVlOXEKAeCrnUzm2JmU1atWlVds3q5/oTBtMz64b+rcNM2/u8dDeYSEWkOkpqM3X26u//G3V9194/c/U6CAVXdCQZ1YWZdgReBTcBPgMOAW4A/mNlvIw5nQKwb3tE3Uiu+ru3meE3tarw56+4PuvsIdx/RtWvXWt5mx/TJacMvogZzPTZpIbOXaTCXiEhTl/J7xu4+DfgW2Dcs+h8gDxjj7i+4+4fufj3BaOqbzaxL2K4I6Gxm0Ymyc0R95HP0lW1OHdsBdIqoT5mfHzKAvjltKr8ud7jh5dkkcxCeiIgkXsqTcSjyKncoMM/do7uLPycYSFVxeTgbaAkMjGpXcW/364h28MM94bjamVke0CaiXcq0arH9ylxTFq7hxWlLUxSRiIgkQsqTsZmNAHYFJodFK4CdzaxzVNP9w+eKzPMmsA04O6rdOcCscOAVwESCAWGx2hUBnwG4+yJgZjXtSoA36v6pGs4Ru3fnyN27VSn78xtzWLdFg7lERJqqrNqbJI6ZPQEsAKYRTFUaDlxLkGDvCZs9QJAQ3zaz24FC4FDgN8C/3X0xgLuvNLM7gGvNbEN4zNMJBoadVPGe7l5iZtcRLPKxFHg3bHMhcIW7R24W/DvgP2b2D+CpML4/AHfVZY5xslx/whA+/m4120rLAVi9cRt3vPMtN46NvvgXEZGmINlXxrMIFtp4BHiLYIWtF4H93X01gLtPAg4CVhGsgPUK8GPgJra/av09weCuq8LjHQCc5u6vRjZy9weAy4DTwnZnAr90979HtXudYNDYyLDdrwhW37qm/h89cfrmtuGyQ6r2zj86sYA5y6OnZouISFOQ1BW40kGiV+CqTnFJGUfd8RGLi7ZUlu2b15lnfz6K7ce0iYhIY9CUVuCSOmjVIpMbTqjaLf1FwRpemqHBXCIiTY2ScRN25ODuHD6o6mCuW1+by/piDeYSEWlKlIybuBtOHEx2xMpcqzdu5c53vkthRCIiEi8l4yauX25bLj14QJWy8RMLmLtCg7lERJoKJeNm4LJDd6Z3p9aVX5eVO9drZS4RkSZDybgZaJ29/cpcny8o4uUZy1IUkYiIxEPJuJk4anB3Dt2t6iYVt74+hw0azCUi0ugpGTcTZsaNJw4hO/OH/9JVG7Zy17sazCUi0tgpGTcjeV3acknUYK5HJhTw7fcbUhSRiIjUhZJxM3P5YbEGc83SYC4RkUZMybiZaZ2dyXUnVB3MNSm/iFdmajCXiEhjpWTcDI0Z0p2Dd606mOtPr89h49bSFEUkIiI1UTJuhoLBXINpkfnDhhHfr9/K3e9pMJeISGMUVzI2s4vNrG1DBSOJM6BrOy4+qOpgroc/XcB3GswlItLoxHtl/ACwzMz+bmZ7NkRAkji/PHxnenVsVfl1qVbmEhFplOJNxgOB+4AfA9PNbKKZnWdmrWp5naRAm+ys7QZzTcwv5D9fLk9RRCIiEktcydjdC9z9WqAPcAawGXiY4Gr5DjPbvQFilHo4Zo8eHLRLlyplt742h00azCUi0mjs0AAudy919+fc/QhgN+BL4Epglpl9ZGbHJzJI2XFmxo1jh1QZzLVifTF3v6/BXCIijcUOj6Y2s/Zm9gvgBeBgYDrweyALeMXMbkpMiFJfA7u246KowVz/+mQB81ZqMJeISGMQdzI2sxFm9k9gGfBXYAYwyt1HuPtt7n4AcCNweUIjlXq54vCd6Rk1mOuGVzSYS0SkMYh3atNUYDJwGHATsJO7n+fuk6OavgN0TkyIkghtsrP4w/FVB3N9Nq+Q179akaKIRESkQrxXxsuAE4Bd3P12dy+qpt00oH+9IpOEO25oDw7YObdK2S2vfa3BXCIiKRZvMr4d+MRj9G2aWTszOxjA3be5+8JEBCiJY2b8cewQsjJ+GMy1fF0x934wL4VRiYhIvMn4A2BwNXW7hfXSiO3crT0/O6hqp8VDn+Qzf9XGFEUkIiLxJmOroa4lUFaPWCRJrjx8F3p0+GEwV0mZc6MGc4mIpExWbQ3MLA+InBczwszaRTVrDVwILEpYZNJg2rbM4vfH784VT02vLPvku9W8OWsFxw7tmcLIRETSU63JGDgPuAHw8HEPVa+QPfy6FE1najJO2LMnT32+iAnzCyvLbv7P1xyyW1faZNfl20JERBKlLt3U4wimMh1BkHR/GX5d8TgcGA30cPd/NkyYkmixBnMtW1fM7178iuIS3W0QEUmmWi+BwlHRCwHM7DBgmrtr6aZmYJfu7bnwwP48+HF+ZdlLM5Yxd8UG7j1rODt3a5/C6ERE0ke8G0V8pETcvFx5xC5VVuYCmLtiAyfc8ylPf75Ig7pERJKg1mRsZvlmNiz894Lw6+oe8xs+ZEmkdi2zeOSCfemX26ZKeXFJOde8+BVXPDWd9cUlKYpORCQ91GWkzkfA+oh/61KpmRnUowP/ueJA/vDSLF6esaxK3X++XM7MJWu5+4zhDO+rFU5FRBqCqRsysUaMGOFTpkxJdRg7xN15fuoSrn95NluiBnFlZRi/GbMblxw0gIyMmqabi4hIdcxsqruPiC7f4S0UpfkxM04d0YdXrziQ3Xt2qFJXWu7c9sZcznvkc1Zt2JqiCEVEmqcd2UJxuJm9aGarzazUzPYOy/9kZsckPkRJtp27tePfvxjN+aPztqv75LvVHHvXJ3z87arkByYi0kzFu4XigcBEYBDwZNTry4FLExeapFKrFpncOHYID567D53atKhSt3rjVn768Ofc9sZcSsrKUxShiEjzEe+V8W3AW8AQ4L+i6qYBeyciKGk8jh7Sg9evPIj98nK2q3vgo/mc+sBEFhdtTkFkIiLNR7zJeG/g/nALxeiRX6uBrgmJShqVXp1a8+TF+3PVEbsQPXZrxuK1HHfXJ7w6c1nsF4uISK3iTcbFQJtq6noC6+oXjjRWWZkZ/OqoXXny4pFVdnwC2LC1lCuems41L3zJlm1aSlNEJF7xJuNPgavNLDOirOIK+WfA+wmJShqtkQNyef2qgzhy927b1T39xWLG3vspc1esj/FKERGpTrzJ+DqCruqZ4b8dOM/MPgBGAn9MbHjSGOW0zeafPx3BDScOJjuz6rfQdys3ctK9n/HYpIVaSlNEpI7iXZt6JnAw8D3we37YxQngEHf/JrHhSWNlZlxwQH9e/MVoBnRpW6Vua2k51700i8sen8a6zVpKU0SkNnHPM3b3ae5+BNAe2Ano4O6Hufv0Wl4qzdAevTvy6hUHcsreO21X9+bsFRx39ydMKShKQWQiIk3HDq/A5e7F7r7M3TWvJc21bZnF304bxh2nD6NtdmaVuqVrt3D6g5O49/3vKCtXt7WISCx12SiiCjMbAJwG9AVaRVW7u/8sEYFJ03Py8J3Yq09nrnxqOl8t/WFgfVm589e3v+WzeYXcecZedO8Q/W0jIpLe4toowsxOAp4juKJeCUQvUuzuPiBx4TU9TXmjiETZVlrO/745l4c+XbBdXU7bbP526jAOG7T9aGwRkeauuo0i4k3GXwHLgbPdXYsTx6Bk/IMP5q7k18/NpGjTtu3qLjqwP/9zzCCys7RXiYikj0Tt2jQA+KsSsdTFYYO68cZVBzFqQO52dQ99uoBT7p9AwepNKYhMRKRxiTcZzwW2/80qUo3uHVrx+EX785ujdyUzai3Nr5au4/i7P+Gl6UtTFJ2ISOMQbzL+H+B34SAukTrJzDB+efguPHPJSHp3al2lbtO2Mq5+Zga/fnYmm7aWpihCEZHUivee8SfAQIKr4++A6Amk7u6HJC68pkf3jGu2bnMJv33hS96cvWK7ugFd2nLPWcMZ0qtjCiITEWl4ibpnXAZ8A0wAVoVfRz60ua3UqGObFtx/zt7c8qM9thu8lb96Eyf/fQKPfLZAS2mKSFqJ68pYaqcr47qbu2I9v3xyOvNWbtyu7sjdu3H7T4bRuW12CiITEWkYiboyFkmYQT068OovD+TM/fpsV/funJUce9cnTMovTEFkIiLJFXcyNrPeZvZ/ZjbFzBaY2R5h+dVmtn8dXn+omXmMx9qINuOqaeNmNjfqeK3M7HYzW25mW8xsopkdHON9M8zsWjMrMLNiM5tpZqdUE+PFZjbXzLaa2Tdmdmm850nqpnV2Jn/+8Z7cc+Zw2resuiDcivXFnPXPSdzxzreUlukOiIg0X3ElYzMbAnwFnAssI1gSs6IfsR9wVRyHuxIYFfE4MqLu5qi6UcCZYd0rUcf5F3AxcD1wAsGiJG+Z2V5R7W4GbgTuBY4FJgHPmdlxUZ/xYuAfwAvAMQQrjt1nZpfF8dkkTicO68XrVx3EXn06VSkvd7jrve8466HJLF+3JUXRiYg0rHhHU79JsFvTGKAY2AaMcPdpZnYq8JfalsM0s0OBD4Cj3P3dON77OuAmYA93nx2WDQNmABe6+yNhWRYwG/jG3ceGZd2AxcBt7n5DxDHfA7q6+54Rr10GvOHu50W0exgYC/R09xr3BNQ94/opKSvnb29/ywMfzd+urlObFtz+k2EcNbh7CiITEam/RN0zPpAgoW0EorP490CPHYyvLn4KTK1IxKGxQAnwTEWBu5cCTwNjzKxlWDyG4Ar+8ahjPg4MNbP+4dejgK4x2j1GMJ3rwAR8DqlBi8wMrjl2EI9euB9d2lUdvLV2cwkXPzqFG1+ZTXFJWYoiFBFJvHiTcU037roA8fQjPmFmZWZWaGZPmlnf6hqa2QHAzsD4qKohwIIY2zjOJki+O0e02wrMi9EOYHBEO4BZtbSTBnbwrl1546qDOWiXLtvVjZtQwMn3TWD+qu1HYYuINEXxJuPPgQuqqTsN+KwOx1gH/A24CDic4F7ukcDEsDs5lp8SXAE/FVWeA6yJ0b4oor7iea1v3ycfqx0xjhndrgozuyQc0DZl1Sot250oXdu3ZPwF+3HNsYPIilpKc87y9Zxw96c8N2Wx5iSLSJMXbzK+GTjRzN4mGMTlwJFmNh44Gbi1tgO4+3R3/427v+ruH7n7nQQDpboTDOqqIuxqPg34j7uvjq5m++7yivIdbUc1bavl7g+6+wh3H9G1a9d4Xiq1yMgwLj1kIM9dOoqdOlddSnNLSRn//fyXXP3MDDYU13grX0SkUYsrGbv7R8CPgP7AwwTJ6zbgIOBH7j55R4Jw92nAt8C+MapPAjqxfRc1BFessa5WO0fUVzx3NrPo5BurHTGOmRNVL0k2vG9nXr/qIE7Ys+d2dS/PWMYJ93zKl0vWxniliEjjF/c8Y3d/zd13AXYlGNC0u7sPcPc36hlLdVev5wGrgddj1M0G+ptZm6jywQQjvedFtGtJsK52dDuAryPawQ/3jqtrJynQoVUL7jlzOH85ZSitWlT91l1YuJlT7p/AQ5/kU16ubmsRaVp2eAUud5/n7hPc/Zv6BmFmIwiS++So8u7A0cCT1UwpegVoAZwa8Zos4HTgbXffGha/SZCcz456/TnALHdfEH49kSDxx2pXRN3uiUsDMjNO37cvr/7yQAb1aF+lrqTMueW1OVw4/gtWb9xazRFERBqfrNqb/MDMflpDdTnB4Kzp7r6khmM8ASwApgFrgeHAtcBS4J6o5meHMcbqosbdZ5jZM8CdZtYiPO5lBN3oZ0e0W2lmdwDXmtmG8L1PJxhAdlJEu5JwPvN9ZrYUeDdscyFwhbtvq+HzSxLt0r09L11+ALe+NofHJi2sUvfhN6s49q5PuPP0vThg5+1HY4uINDbxLvpRzg9dyZH3XyPLygnm/V4QK3mZ2bUEq2n1A9oAK4A3gBvcfXlU25lAhrsPrSGm1gQDx84iuLc8E/itu38Y1S6TIOlfTDAf+hvgJnd/PsYxfw78OoxxEXCHu99XXQyRtOhH8r05azn/8/yXrC+uuh+yGfzi0IH86shdycrUMuwiknrVLfoRbzIeBTwBvAo8T7DQR3eC0c4nAL8A9gD+CNzp7r+rf+hNi5JxaixZs5mrn57BlIXbz3Tbp19n7jpjL3bqHD20QEQkuRKVjF8gWGZyuyRrZn8iGMx1spndDJxd29KYzZGSceqUlpVz13vfce8H84j+tm7VIoMf7dWb80bnsXvPDqkJUETSXqKWwzwKeK+auveBI8J/fwz0jvPYIvWSlZnBr4/ejScu2p9u7VtWqSsuKefpLxZz7F2fcPo/JvLmrOXaCUpEGo14k/E2YJ9q6vYJ6yuOu2lHgxKpj9EDu/DGVQdx2G6xF2CZvKCISx+fxiG3f8j9H85nzSaNyxOR1Io3GT8H/NHMfm1m/cysdfj8G4LtCSs2bNiLYICUSErktmvJw+fvyy0/2oPenVrHbLN07RhYlFMAAB4aSURBVBb+8uZcRv75PX77/Jd8vWx9kqMUEQnEe8+4NfBPfthbONKTwMXuXmxmxwMb3P3jxITZdOieceNTWlbOu3NWMm7CAibl17yI2n79czh/dB5HD+6uEdgiknAJGcAVcbBdgZEEU4SWA5Pd/dt6R9kMKBk3bnOWr+fRiQX8e/pSikuqv2fcq2Mrzh7ZjzP360tO2+xq24mIxCOhyViqp2TcNKzdvI1nvljMoxMXsnRt9Tt/ZmdlcNKwXpw3Oo89endMYoQi0hwlLBmH60BfCBxCsIFCIfAhMC7GvsJpR8m4aSkrd96d8z3jJxQwYX5hjW33zevM+aP7c/SQ7rRQF7aI7IBEzTPuQZB4dwUWEqye1YNgpapvgEPd/ftEBNxUKRk3Xd+s2MD4iQW8OG1JjV3YPTq04txR/Thj3z7ktmtZbTsRkWiJSsaPAmOAH7v7ZxHlo4EXgLfc/fz6h9t0KRk3fes2l/DslMWMn1jAkjU1d2GPHdaL89WFLSJ1lKhkvIpg3eeHY9T9DLjN3WNP7kwTSsbNR1m58/7cYBT2Z/Nq7sIe0a8z543O45g9eqgLW0SqVV0yjmvXJqAdsKyauiVhvUizkJlhHDW4O0cN7s63329g/IQCXpy2lC0lZdu1nbJwDVMWrqF7h5acs38/zty/L13UhS0idRTvlfEMYLa7R+/3i5k9Buzh7sMTGF+Toyvj5m3d5hKemxqMwl5UVP14xezMDE4Y1pMLRvdn6E7qwhaRQKK6qc8BHiVYh/pJgjnGPYAzgCOBc939yYRE3EQpGaeHsnLng7krGT+xgE++W11j2737duL8A/pzrLqwRdJeIqc2XQLcBHSLKP4euN7d/1mvKJsBJeP0M2/lBsZPWMgL05awedv2XdgVurVvyTnhQiJd26sLWyQdJXoFrgxgN4J5xkUE2ypqCxyUjNPZui0lPD91CeMnFNTehb1nT84bncewPp2SGKGIpFq9k7GZZQOTgGvc/e0Ex9dsKBlLebnz4bcreeSz2ruwh/ftxPmj8zh2j55kZ6kLW6S5S9Q94zXAKe7+fiKDa06UjCXSvJUbeXRiAS9MXcKmGrqwu7Zvydn79+Ws/fvSrX2r5AUoIkmVqGT8LJDv7tckMrjmRMlYYllfXMLzU5bw6MQCCgqr78JukWkcPzTowh7et3PyAhSRpEhUMj4IeJxgX+OXCEZTVzmAu+fXL9SmTclYalJe7nz07SrGTSjgo29X1dh2WJ9OXDA6j+OGqgtbpLlIVDKOHKQV84Xunhl/eM2HkrHU1fxVG3ls4kKem7K4xi7sLu1actb+fTln/75066AubJGmLFHJ+Lza2rj7+Dhja1aUjCVeG4pLeGHqEsZPXMiC1ZuqbZeVYRw3tCfnH5DH8D6dMLMkRikiiaD9jJNEyVh2VHm58/F3QRf2h9/U3IW9504dOX90Hsfv2ZOWWWndGSXSpDTEPOPBQC4wxd2r/3M+zSgZSyLkr9rIoxMX8vzUJWzcWlptuy7tsjlrv76cPbIf3dWFLdLoJXIFrsuBG4AuBPeN93X3aWb2EvC+u9+diICbKiVjSaSNW0vDLuwC8lfV3IV97NCenD+6H3v37awubJFGKlH3jC8G7gceBt4GngVGhMn418BYdz8kQTE3SUrG0hDKy51P5q1m/IQC3p+7ssa2Q3sHXdgnDFMXtkhjk6hkPAd4xd1/a2aZQAk/JOPjgX+5e4+ERd0EKRlLQytYvYlHw1HYG2rows5tm82Z+/XlnJH96NFRXdgijUGiknExcJy7vx8jGR8KvOnuaf1Tr2QsybJxayn/nraEcRMKmF9LF/aYPXpwweg89umnLmyRVKouGWfFeZzVQF41dbsBS+M8nojsoHYtszh3VB7njOzHp/NWM+6zAt7/ZiXRf1+Xljuvfbmc175czpBeHTh/dB4nDutFqxbqwhZpLOK9Mr4fOBY4HFhIcGW8D7AY+BR4zd1/3QBxNhm6MpZUWlgYdGE/O2UxG4qr78LOaZvNmfv14ZyR/ejZsXUSIxRJb4nqps4FJgB9gMnAweHXg4CVwGh3X5eQiJsoJWNpDDZtLeXF6UsZP6GAeSs3VtsuM8M4ZkgPzhudx7556sIWaWiJnNrUHrgaGAN0AwqBN4E73H19AmJt0pSMpTFxdz6bV8i4CQW8N/f77bqwIw3uGXRhj91LXdgiDUUrcCWJkrE0VosKN/PYpAKe/qLmLuzObVpwxn59OXdkP3p1Uhe2SCIlqps6HzjZ3WfGqNuDYNrTgHpF2sQpGUtjt3lbKf+evpRxnxXwXS1d2EcP7s75o/PYr3+OurBFEiCRuzaNdPfPY9SNACZr1yYlY2ka3J2J8wt5ZEIB786puQt7UI/2XHBAHift1Vtd2CL1kMhkvL+7fxGj7lLgVnfPrVekTZySsTRFi4s289ikhTz9+SLW19CF3alNC87Yty/njupHb3Vhi8Rth5Oxmf0K+FX4ZW9gFbAtqllrIAd42t3Prn+4TZeSsTRlm7eV8tL0ZYyfUMA332+otl2GwdGDg1HYIweoC1ukruqTjE8CfhR+eR7wOkFCjrQV+Bp4yN031z/cpkvJWJoDd2difiHjJxTwztffU15LF/Z5o/P40V69aZ2tLmyRmiSqm/oR4CZ3X5DI4JoTJWNpbhYXbebxyQt5+vPFrNtSUm27jq1bcMa+fTh1RB8Gdm2rq2WRGBI+tcnM2hHsZ7zM3av/CU0zSsbSXG3ZVsbLM5YybkIBc1dU34UN0KNDK0YNzGXUgFxGDcylT06bJEUp0rglctGPE4CbgGFhUcV+xg8R7Gf8ZL2jbcKUjKW5c3cmLyhi3GcFvP31ihq7sCv07tSa0QODxDxqYK6W4JS0lahu6h8BLwDvEexn/L/8sGvT74GD3X1MgmJukpSMJZ0sWbOZxyct4ukvFrF2c907yPJy2zBqYBdGDcxl5IAcurVP683eJI0kKhlPB6a6+0VmlkUwqroiGZ8E3OfuvRMWdROkZCzpqLgk6ML+z5fL+aKgiOKS8rhev3O3dowakMvogbnsPyCXnLbZDRSpSGolcj/jE939nRj7GR8MvK39jJWMJb1tLS1j5uJ1TJxfyMT81UxbtJZtpfEl50E92lfec95/QC4dW7dooGhFkitR+xmvB7pUU5fH9lOeRCTNtMzKZL/+OezXP4er2IXikjKmLVoTJOf5hcxYvJbSWm40z12xgbkrNvDIZwVkGAzp1bEyOe/bP4d2LeP91SXSuMV7ZfwEMJRg68QN/LCf8dfAJ8AMd7+kAeJsMnRlLFKzzdtKmVKwhon5hUyYX8hXS9bWaRBYhcwMY8+dOlaO1B7RL0fzm6XJSFQ3dR7wOeAEi3/8FHge2BPoSNBlvSwB8TZZSsYi8dlQXMIXBUVMnB8k56+Xr69xnexoLTKN4X06MzK8ch7et5PWz5ZGK5FTm3YC/sj2+xlf7+6LExBrk6ZkLFI/azdvY/KCospu7ZqW5YylZVYGe/ftXDmVas+dOpGdldFA0YrER/sZJ4mSsUhiFW7cyqT8Iibmr2bi/ELmr9oU1+tbt8hkRF5nRg3MZfTALuzRqwNZmUrOkhr1WZv6+jjex9395niDa06UjEUa1vfri5mUXxiO1i5kYWF8y+G3a5nFfv1zKu85796zA5kZWrpTkqM+yTjWnAQHYn33uvYzVjIWSaala7dUdmlPyi9k6dotcb2+Y+sW7N8/p3J1sF27tSdDyVkaSH2ScXRyzQK2APsD06Lbu3tZPeJs8pSMRVLH3VlctIWJ+auZECbolRu2xnWMnLbZjByQE6wQNiBXm15IQiVyAFeVxT4SFF+zoWQs0ni4O/mrN1W5ci7cFL0de826tW/JyHB1sFEDc+mb00bJWXaYknGSKBmLNF7uzrffb2Ti/NVMzC9kUn5RjdtCxtKrY6vKaVSjBuayU2ftSCV112iSsZkdCnwQo2qdu3eKajsSuBEYCbQA8oFb3f3piDatgJuBc4BOwAzgt+7+cdSxMoDfAj8HegDfEOzN/EKMGC8Gfg30BwqAO9z9gbp8PiVjkaajvNz5evn6ygFhny8oYsPW0riO0TenTWViHjUwl+4d0npFYKlFopbDTKQrgS8ivq7yE2BmxwP/Bp4EziLYlGIwEP2d/i/geOC/CZL15cBbZjbK3WdEtLsZ+A3we2AqcAbwnJmd4O6vR7zvxcA/gD8D7wJHAPeZmbn7/fX6xCLSqGRkGHv07sgevTty0UEDKC0rZ9ay9ZUjtacUFLF5W83DYBYVbWZR0WaemRIsszCga9vK5DxyQC5d2rVMxkeRJq4uA7gGRBVlElxVngTMjm7v7vm1HO9Qgivjo9z93WratAfmA0+6+9U1HGsYwZXwhe7+SFiWFcb1jbuPDcu6AYuB29z9hojXvwd0dfc9I167DHjD3c+LaPcwMBbo6e419mnpylik+SgpK+fLJWuZMC9IzlMXrmFrnJte7Nq9HaMHdmHkgGC7yE5ttCNVOqvPlfE8gqlM0V6qpn0ipjadCnQF/lZLu7EEXebPVBS4e6mZPQ1cY2Yt3X0rwWph2cDjUa9/HHjYzPq7+wJgVPi+0e0eAy4ADiR2F7uINEMtMjPYp18O+/TL4Yojgk0vZixey4T5hUyaX8j0xWsoKav5gubb7zfy7fcbGTehADPYvUeHcAGSYNOLDq20I5XULRlf0EDv/YSZdQHWAm8B17j7orDuQKAIGGpmrwO7A8uBh4BbIqZPDQEWuHv0rP/ZBMl35/DfQ4CtBH9YRLeDoPt7QdgOYFYN7ZSMRdJUqxaZ4RVuLhwFW7aVMXXhmsqpVF8uWUdZDbteuMPXy9fz9fL1/OvTBWQYDO3dkZHh6mAj+nWmrXakSku1/q+7+/gEv+c6givejwi2ZBwO/A6YaGbD3X0l0AtoQ3C/+GaCe7xHAtcRDNL6VXisHGBNjPcoiqiveF7r2/fJx2pHjGNGt6vCzC4BLgHo27dvrCYi0gy1zs7kwF26cOAuwc6yG7eW8kVBEZPCe86zlq6rcUeqcoeZS9Yxc8k6/vFRPlkZxrA+nSrvOe/Tr7M2vUgTSf8TzN2nA9Mjij4ys48JdoO6EvgDkEEwUOv37v5/YbsPzSwXuNzMbnT3dQSrgMX6Vo+eBBhPO6ppWy13fxB4EIJ7xvG8VkSaj3Ytszhst24ctls3ANZtKeHzik0v8guZs3x9ja8vLXemLlzD1IVruPeDeWRnZjC8b6fKvZz36tuJlllKzs1Ro+gPcfdpZvYtsG9YVBg+vxPV9G3gUoLu5AkEV6yxLkU7h89FEc+dwxHRXks7CK6Al0e0y4mqFxGpVcfWLThqcHeOGtwdgKJN25icHyTmifML+W7lxhpfv62snMkLipi8oIg7+Y5WLTIY0S+ncqT2njt1pIU2vWgWGkUyDkVevVbco42+yqy4ci2PaHeymbWJum88mGAq1LyIdi2BgVS9bzw4fP466n2HUDUZR7cTEYlbTttsjh3ak2OH9gRg5YbiYEeqcHWwBatr3pGquKScT+et5tN5qwFom53JiLycytXBhvTqqE0vmqhGkYzNbASwK/BsWPQSwb3iY6g6mGoMUBxR9grB3sqnAuPDY2UBpwNvhyOpIdhveRtwdti+wjnArHAkNcBEYHXY7t2odkXAZ/X5nCIikbq1b8XYYb0YO6wXAMvXbWFSfmHlVKola2re9GLTtjI++nYVH327CoD2rbLCTS+CdbUH9dCmF01F0pOxmT1BMHJ5GsFI6uHAtcBS4B4Ad59lZuOAm8KVs6YRDOC6CLjZ3TeG7WaY2TPAnWbWIjzuZQQrZ51d8Z7uvtLM7gCuNbMN4fFOBw4nmC9d0a7EzK4jWORjKUFCPhy4ELjC3eNb1FZEJA49O7bm5OE7cfLwnQBYXLQ5WLZzfiET5heyYn1xja/fUFzKu3NW8u6clQB0btOC/fv/sDrYLt3aaV3tRiru5TDr/YZm1wJnAv0IRkyvAN4AbnD35RHtsoHrgfOA7gTLUv7d3e+KOl5r4FaCVbo6ATMJlsP8MKpdJkHSv5iqy2E+HyPGnxMsh9kPWESwHOZ9dfl8WvRDRBqCu1NQuLlyMNjE+YWs3hjfjlRd2rUMd6QKBoT176IdqZItYWtTS82UjEUkGdydeSs3VibmSfmFrNkc36YXPTq0qkzMowbm0idHm140NCXjJFEyFpFUKC935q7YUJmcJy8oZENxfJte9O7UunJ1sFEDc+nZsXUDRZu+lIyTRMlYRBqDsnLn62XrmRBuF/nFgiI21bLpRbS83DaV06hGDcylW3vtSFVfSsZJomQsIo1RSVk5Xy1dF9xznl/IlIVFFJfEt+nFzt3aVdmRKqetNr2Il5JxkigZi0hTsLW0jJmLg+Q8Yf5qpi9ay7ay+JLzoB7tK+857z8gl46ttelFbZSMk0TJWESaouKSMqYtXMPE/GAa1czFaymtaWHtKGawR6+Olcl53/45tNOmF9tRMk4SJWMRaQ42bS1lysI1lVOpvlqytsZNL6JlZhhDe3esHAw2ol8OrbO1rraScZIoGYtIc7S+uIQvIja9+Hr5euJJHy0yjb0qd6TqwvC+ndJyRyol4yRRMhaRdLB28zYm5RcxKZxK9c33G+J6fXZWBvv07Vw5lWrPnTqRndX8N71QMk4SJWMRSUerN25lcn5R5VSq/FU1b3oRrXWLTEbkda685zy0d0eymuGOVErGSaJkLCIC368vrrLpxaKizbW/KEK7llns1z+ncirV7j07NIsdqZSMk0TJWERke0vXbqmc4zxx/mqWrat504toHVu3CHekCpLzrt2a5o5USsZJomQsIlIzd2dRUdVNL1ZuiG/Ti5y22eGmF8F2kQO7No1NL5SMk0TJWEQkPu7O/FWbKreLnJRfSOGm+Has7da+ZeWynaMG5NIvt02jTM5KxkmiZCwiUj/l5c53KzcGg8HmFzJ5QRHrtsS3I1Wvjq0YGbEj1U6dG8eOVErGSaJkLCKSWGXlzpzl6yu7tT9fUMTGrfHtSNU3p01lYh41MJfuHVKz6YWScZIoGYuINKzSsnJmLVtfua72lII1bCmJb0eqAV3aVibmkQNy6dKuZQNFW5WScZIoGYuIJNe20nK+XLK28sp5ysI1bCuNb9OLXbu3q1wdbOSAHDq1aZgdqZSMk0TJWEQktYpLypi+aG3lgLDpi9dQUhbfphe79+hQORhsvwE5dGiVmB2plIyTRMlYRKRx2bKtjCkLf1hX+8sl6yiLY9eLDIOhvTsycmAulx0ysF5XzdUlY+1vJSIizVrr7EwO2qUrB+3SFYCNW0uDTS/COc6zlq2rcdOLcoeZS9YxZ/kGfnXkrg0So5KxiIiklXYtszhsUDcOG9QNgHVbSvh8QVHlVKq5K2JvetGQO00pGYuISFrr2LoFRw3uzlGDuwNQtGkbk/ODLu0J8wuZt3IjAKMG5jZYDErGIiIiEXLaZnPs0J4cO7QnACs3FDMpv4jBPTs02HsqGYuIiNSgW/tWjB3Wq0Hfo/ltFikiItLEKBmLiIikmJKxiIhIiikZi4iIpJiSsYiISIppOcwEM7NVwMI6Nu8CrG7AcJo6nZ/a6RzVTueodjpHtUvUOern7l2jC5WMU8jMpsRao1QCOj+10zmqnc5R7XSOatfQ50jd1CIiIimmZCwiIpJiSsap9WCqA2jkdH5qp3NUO52j2ukc1a5Bz5HuGYuIiKSYroxFRERSTMlYREQkxZSMk8jM+pjZ82a2zszWm9mLZtY31XGlgpn9xMxeMLOFZrbFzL4xsz+bWfuodp3N7CEzW21mm8zsXTMbmqq4U83M3jQzN7NbosrT+jyZ2XFm9rGZbQx/tqaY2eER9el+fg4ws7fNbGV4fqaZ2YVRbVqZ2e1mtjz8mZxoZgenKuaGYmY7mdk94efbHP485cVoV6fzYWYZZnatmRWYWbGZzTSzU+KNS8k4ScysDfA+MAg4DzgX2AX4wMzapjK2FPkNUAb8DjgGuB+4DHjHzDIAzMyAV8L6K4BTgBYE52ynVASdSmZ2JjAsRnlanycz+znwMjAVOBk4FXgOaBPWp/v52RN4l+AzX0zw+b8A/mVml0U0/VdYfz1wArAceMvM9kpuxA1uZ+A0YA3wSQ3t6no+bgZuBO4FjgUmAc+Z2XFxReXueiThAVxFkHx2jijrD5QC/5Xq+FJwPrrGKPsp4MDh4dcnhV8fFtGmI1AE3J3qz5Dk89UJWAGcGZ6TWyLq0vY8AXnAFuDqGtqk7fkJP+ufgG1Au6jyScDE8N/DwnN0QUR9FvAN8EqqP0OCz0dGxL8vCj93XlSbOp0PoBuwFfhj1OvfA76MJy5dGSfPWGCSu8+rKHD3BcBnBL8s0oq7r4pR/EX43Dt8Hgssc/cPIl63DniV9Dtn/wvMdvenYtSl83m6ECgHHqihTTqfH4BsoITgj5ZIa/mhd3Rs2OaZikp3LwWeBsaYWcskxJkU7l5eh2Z1PR9jCM7v41GvfxwYamb96xqXknHyDAFmxSifDQxOciyN1SHh85zwuaZz1tfM2iUlqhQzswMJeg1+UU2TdD5PBwJzgTPMbL6ZlZrZPDO7PKJNOp8fgHHh891m1svMOpnZxcARwB1h3RBggbtvjnrtbIJks3NSIm086no+hhBcGc+L0Q7i+N2uZJw8OQT3KKIVAZ2THEujY2a9gZuAd919Slhc0zmDNDhvZtYC+AfwV3f/pppm6XyeehGMvbgduA04GngHuNfMrgrbpPP5wd1nAYcS9AIsJTgXfwcudfenw2a1naOcBg6zsanr+cgB1nrYN11Du1plxRWe1FesFVYs6VE0MuGVycsE988viKxC5+y3QGvg1hrapPN5ygDaA+e7+4th2fvh6Nhrzexu0vv8YGa7AC8QXK1dStBdfRLwgJkVu/sTpPk5iqGu5yNh503JOHnWEPuvpM7E/gssLZhZK4KRrgOAQ9x9SUR1EdWfM2jm5y2c9vZ7gkEmLaPu27U0s07ABtL7PBUSXBm/E1X+NsHo6Z6k9/mBYABXCXCCu5eEZe+ZWS5wl5k9RXCOYk2zrDhHRTHqmrO6no8ioLOZWdTVcdznTd3UyTOb4P5CtMHA10mOpVEIu2BfAPYDjnP3r6Ka1HTOFrn7xgYOMdUGAK0IBoOsiXhAMDVsDTCU9D5Ps6spr7gyKSe9zw8E3yMzIxJxhc+BXIIRwbOB/uEUzEiDCUZiR98Tbe7qej5mAy2BgTHaQRy/25WMk+cVYKSZDagoCLvSDgjr0ko4l/gJgkEkJ7n7pBjNXgF6m9khEa/rAJxIepyzGcBhMR4QJOjDCH4ppPN5+nf4PCaqfAywxN1XkN7nB4IpcXuZWXZU+f5AMcHV2ysE85BPrag0syzgdOBtd9+apFgbi7qejzcJkvPZUa8/B5gVzpipm1TP+UqXB9CW4BfnVwT3a8YCM4F8oub/pcODYJEPB24BRkY9dgrbZAATgMXAGQS/YD8k+OXRJ9WfIYXnLnqecdqeJ4Ir4PcJuqsvJRjA9WB4js5P9/MTfv6fhOfjrfB3z9EEC1Q48H8R7Z4m6G25iOCP5OcJkvXeqf4MDXROfhLxe+iy8OtD4j0fBAMHi4H/Ihgodz9Bj8yJccWU6pOSTg+CexAvAOsJ7vW9RNRk83R5AAXhD0Gsx40R7XKAh8NfnJsJJtMPS3X8KT53VZJxup8noAPB6ODvCa5SvgTO0vmp8vmPDf8AWRX+7plBMFUuM6JNa+D/CK6ki4HJwKGpjr2Bzkd1v3s+jPd8AJnAH4CFBNOcvgR+Em9M2kJRREQkxXTPWEREJMWUjEVERFJMyVhERCTFlIxFRERSTMlYREQkxZSMRUREUkzJWEQSyszONzM3s52jyvc1syIzm25mXVIVn0hjpGQsIg3OzEYD7wLfAYe7++oUhyTSqCgZi0iDCteEfotgKdij3L2575IkEjclYxFpMGZ2FPAG8AUwxt3XpzgkkUZJyVhEGsrxwKvAx8Dx7r4pxfGINFpKxiLSUO4ElhBskbkl1cGINGZKxiLSUF4j2HT92lQHItLYZaU6ABFptn5FsP3cDWZW7O63pTogkcZKyVhEGooDlwAtgT+HCfnOFMck0igpGYtIg3H3cjM7H8gG7ggT8gMpDkuk0VEyFpEG5e5lZnY2wRXyfWa21d0fSXVcIo2JBnCJSINz91LgNOBN4CEzOyvFIYk0KubuqY5BREQkrenKWEREJMWUjEVERFJMyVhERCTFlIxFRERSTMlYREQkxZSMRUREUkzJWEREJMWUjEVERFLs/wFExS1d4tvoKQAAAABJRU5ErkJggg==\n",
      "text/plain": "<Figure size 504x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"268.399pt\" version=\"1.1\" viewBox=\"0 0 484.475 268.399\" width=\"484.475pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 268.399 \nL 484.475 268.399 \nL 484.475 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 84.585 219.229 \nL 477.275 219.229 \nL 477.275 27.789 \nL 84.585 27.789 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m77a34db2d0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"95.149017\" xlink:href=\"#m77a34db2d0\" y=\"219.229\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(90.059017 238.3865)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"168.004304\" xlink:href=\"#m77a34db2d0\" y=\"219.229\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(157.824304 238.3865)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"240.859592\" xlink:href=\"#m77a34db2d0\" y=\"219.229\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(230.679592 238.3865)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"313.714879\" xlink:href=\"#m77a34db2d0\" y=\"219.229\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(303.534879 238.3865)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"386.570167\" xlink:href=\"#m77a34db2d0\" y=\"219.229\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(376.390167 238.3865)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"459.425455\" xlink:href=\"#m77a34db2d0\" y=\"219.229\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(444.155455 238.3865)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- K -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 42.09375 \nL 52.390625 72.90625 \nL 65.09375 72.90625 \nL 28.90625 38.921875 \nL 67.671875 0 \nL 54.6875 0 \nL 19.671875 35.109375 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-75\"/>\n     </defs>\n     <g transform=\"translate(275.68375 257.8715)scale(0.16 -0.16)\">\n      <use xlink:href=\"#DejaVuSans-75\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mbe5faca764\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"84.585\" xlink:href=\"#mbe5faca764\" y=\"182.835164\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 56000 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(26.685 188.913914)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"84.585\" xlink:href=\"#mbe5faca764\" y=\"115.691626\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 57000 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(26.685 121.770376)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"84.585\" xlink:href=\"#mbe5faca764\" y=\"48.548087\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 58000 -->\n      <g transform=\"translate(26.685 54.626837)scale(0.16 -0.16)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- Heterogeneity -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 43.015625 \nL 55.515625 43.015625 \nL 55.515625 72.90625 \nL 65.375 72.90625 \nL 65.375 0 \nL 55.515625 0 \nL 55.515625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-72\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <g transform=\"translate(19.3575 180.77525)rotate(-90)scale(0.16 -0.16)\">\n      <use xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"75.195312\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"136.71875\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"175.927734\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"237.451172\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"278.533203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"339.714844\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"403.191406\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"464.714844\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"528.09375\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"589.617188\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"617.400391\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"656.609375\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_10\">\n    <path clip-path=\"url(#p6738d5d2bc)\" d=\"M 102.434545 36.490818 \nL 131.57666 85.184337 \nL 186.218126 125.180465 \nL 277.287236 164.133045 \nL 459.425455 210.527182 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:4;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 84.585 219.229 \nL 84.585 27.789 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 477.275 219.229 \nL 477.275 27.789 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 84.585 219.229 \nL 477.275 219.229 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 84.585 27.789 \nL 477.275 27.789 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_12\">\n    <!-- K vs. Heterogeneity -->\n    <defs>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n    </defs>\n    <g transform=\"translate(186.079 21.789)scale(0.192 -0.192)\">\n     <use xlink:href=\"#DejaVuSans-75\"/>\n     <use x=\"65.576172\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"97.363281\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"156.542969\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"208.642578\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"240.429688\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"272.216797\" xlink:href=\"#DejaVuSans-72\"/>\n     <use x=\"347.412109\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"408.935547\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"448.144531\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"509.667969\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"550.75\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"611.931641\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"675.408203\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"736.931641\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"800.310547\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"861.833984\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"889.617188\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"928.826172\" xlink:href=\"#DejaVuSans-121\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6738d5d2bc\">\n   <rect height=\"191.44\" width=\"392.69\" x=\"84.585\" y=\"27.789\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plot_k_vs_heterogeneity(k_list, heterogeneity_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot we show that heterogeneity goes down as we increase the number of clusters. Does this mean we should always favor a higher K? **Not at all!** As we will see in the following section, setting K too high may end up separating data points that are actually pretty alike. At the extreme, we can set individual data points to be their own clusters (K=N) and achieve zero heterogeneity, but separating each data point into its own cluster is hardly a desirable outcome. In the following section, we will learn how to detect a K set \"too large\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize clusters of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start visualizing some clustering results to see if we think the clustering makes sense.  We can use such visualizations to help us assess whether we have set K too large or too small for a given application.  Following the theme of this course, we will judge whether the clustering makes sense in the context of document analysis.\n",
    "\n",
    "What are we looking for in a good clustering of documents?\n",
    "* Documents in the same cluster should be similar.\n",
    "* Documents from different clusters should be less similar.\n",
    "\n",
    "So a bad clustering exhibits either of two symptoms:\n",
    "* Documents in a cluster have mixed content.\n",
    "* Documents with similar content are divided up and put into different clusters.\n",
    "\n",
    "To help visualize the clustering, we do the following:\n",
    "* Fetch nearest neighbors of each centroid from the set of documents assigned to that cluster. We will consider these documents as being representative of the cluster.\n",
    "* Print titles and first sentences of those nearest neighbors.\n",
    "* Print top 5 words that have highest tf-idf weights in each centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_document_clusters(wiki, tf_idf, centroids, cluster_assignment, k, map_word_to_index, display_content=True):\n",
    "    '''wiki: original dataframe\n",
    "       tf_idf: data matrix, sparse matrix format\n",
    "       map_index_to_word: SFrame specifying the mapping betweeen words and column indices\n",
    "       display_content: if True, display 8 nearest neighbors of each centroid'''\n",
    "    map_index_to_word =  {v:k for k,v in map_word_to_index.items()}\n",
    "    print('==========================================================')\n",
    "    # Visualize each cluster c\n",
    "    for c in range(k):\n",
    "        # Cluster heading\n",
    "        print('Cluster {0:d}    '.format(c)),\n",
    "        # Print top 5 words with largest TF-IDF weights in the cluster\n",
    "        idx = centroids[c].argsort()[::-1]\n",
    "        for i in range(5): # Print each word along with the TF-IDF weight\n",
    "            print('{0:s}:{1:.3f}'.format(map_index_to_word[idx[i]], centroids[c][idx[i]])),\n",
    "        print('')\n",
    "        \n",
    "        if display_content:\n",
    "            # Compute distances from the centroid to all data points in the cluster,\n",
    "            # and compute nearest neighbors of the centroids within the cluster.\n",
    "            distances = pairwise_distances(tf_idf, centroids[c].reshape(1, -1), metric='euclidean').flatten()\n",
    "            distances[cluster_assignment!=c] = float('inf') # remove non-members from consideration\n",
    "            \n",
    "            nearest_neighbors = distances.argsort()\n",
    "            \n",
    "            # For 8 nearest neighbors, print the title as well as first 180 characters of text.\n",
    "            # Wrap the text at 80-character mark.\n",
    "            for i in range(8):\n",
    "                text = ' '.join(wiki[nearest_neighbors[i]]['text'].split(None, 25)[0:25])\n",
    "                print('\\n* {0:50s} {1:.5f}\\n  {2:s}\\n  {3:s}'.format(wiki[nearest_neighbors[i]]['name'],\n",
    "                    distances[nearest_neighbors[i]], text[:90], text[90:180] if len(text) > 90 else ''))\n",
    "        print('==========================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first look at the 2 cluster case (K=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "==========================================================\nCluster 0    \nshe:0.025\nher:0.017\nmusic:0.012\nhe:0.011\nuniversity:0.011\n\n\n* Anita Kunz                                         0.97401\n  anita e kunz oc born 1956 is a canadianborn artist and illustratorkunz has lived in london\n   new york and toronto contributing to magazines and working\n\n* Janet Jackson                                      0.97472\n  janet damita jo jackson born may 16 1966 is an american singer songwriter and actress know\n  n for a series of sonically innovative socially conscious and\n\n* Madonna (entertainer)                              0.97475\n  madonna louise ciccone tkoni born august 16 1958 is an american singer songwriter actress \n  and businesswoman she achieved popularity by pushing the boundaries of lyrical\n\n* %C3%81ine Hyland                                   0.97536\n  ine hyland ne donlon is emeritus professor of education and former vicepresident of univer\n  sity college cork ireland she was born in 1942 in athboy co\n\n* Jane Fonda                                         0.97621\n  jane fonda born lady jayne seymour fonda december 21 1937 is an american actress writer po\n  litical activist former fashion model and fitness guru she is\n\n* Christine Robertson                                0.97643\n  christine mary robertson born 5 october 1948 is an australian politician and former austra\n  lian labor party member of the new south wales legislative council serving\n\n* Pat Studdy-Clift                                   0.97643\n  pat studdyclift is an australian author specialising in historical fiction and nonfictionb\n  orn in 1925 she lived in gunnedah until she was sent to a boarding\n\n* Alexandra Potter                                   0.97646\n  alexandra potter born 1970 is a british author of romantic comediesborn in bradford yorksh\n  ire england and educated at liverpool university gaining an honors degree in\n==========================================================\nCluster 1    \nleague:0.040\nseason:0.036\nteam:0.029\nfootball:0.029\nplayed:0.028\n\n\n* Todd Williams                                      0.95468\n  todd michael williams born february 13 1971 in syracuse new york is a former major league \n  baseball relief pitcher he attended east syracuseminoa high school\n\n* Gord Sherven                                       0.95622\n  gordon r sherven born august 21 1963 in gravelbourg saskatchewan and raised in mankota sas\n  katchewan is a retired canadian professional ice hockey forward who played\n\n* Justin Knoedler                                    0.95639\n  justin joseph knoedler born july 17 1980 in springfield illinois is a former major league \n  baseball catcherknoedler was originally drafted by the st louis cardinals\n\n* Chris Day                                          0.95648\n  christopher nicholas chris day born 28 july 1975 is an english professional footballer who\n   plays as a goalkeeper for stevenageday started his career at tottenham\n\n* Tony Smith (footballer, born 1957)                 0.95653\n  anthony tony smith born 20 february 1957 is a former footballer who played as a central de\n  fender in the football league in the 1970s and\n\n* Ashley Prescott                                    0.95761\n  ashley prescott born 11 september 1972 is a former australian rules footballer he played w\n  ith the richmond and fremantle football clubs in the afl between\n\n* Leslie Lea                                         0.95802\n  leslie lea born 5 october 1942 in manchester is an english former professional footballer \n  he played as a midfielderlea began his professional career with blackpool\n\n* Tommy Anderson (footballer)                        0.95818\n  thomas cowan tommy anderson born 24 september 1934 in haddington is a scottish former prof\n  essional footballer he played as a forward and was noted for\n==========================================================\n"
    }
   ],
   "source": [
    "'''Notice the extra pairs of parentheses for centroids and cluster_assignment.\n",
    "   The centroid and cluster_assignment are still inside the npz file,\n",
    "   and we need to explicitly indicate when to load them into memory.'''\n",
    "visualize_document_clusters(wiki, tf_idf, centroids[2], cluster_assignment[2], 2, map_index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both clusters have mixed content, although cluster 1 is much purer than cluster 0:\n",
    "* Cluster 0: academia, law\n",
    "* Cluster 1: female figures, baseball players\n",
    "\n",
    "Roughly speaking, the entire dataset was divided into athletes and non-athletes. It would be better if we sub-divided non-atheletes into more categories. So let us use more clusters. How about `K=10`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "==========================================================\nCluster 0    \nfilm:0.020\nart:0.014\nhe:0.011\nbook:0.010\ntelevision:0.010\n\n\n* Wilson McLean                                      0.97479\n  wilson mclean born 1937 is a scottish illustrator and artist he has illustrated primarily \n  in the field of advertising but has also provided cover art\n\n* Anton Hecht                                        0.97748\n  anton hecht is an english artist born in london in 2007 he asked musicians from around the\n   durham area to contribute to a soundtrack for\n\n* David Salle                                        0.97800\n  david salle born 1952 is an american painter printmaker and stage designer who helped defi\n  ne postmodern sensibility salle was born in norman oklahoma he earned\n\n* Vipin Sharma                                       0.97805\n  vipin sharma is an indian actor born in new delhi he is a graduate of national school of d\n  rama new delhi india and the canadian\n\n* Paul Swadel                                        0.97823\n  paul swadel is a new zealand film director and producerhe has directed and produced many s\n  uccessful short films which have screened in competition at cannes\n\n* Allan Stratton                                     0.97834\n  allan stratton born 1951 is a canadian playwright and novelistborn in stratford ontario st\n  ratton began his professional arts career while he was still in high\n\n* Bill Bennett (director)                            0.97848\n  bill bennett born 1953 is an australian film director producer and screenwriterhe dropped \n  out of medicine at queensland university in 1972 and joined the australian\n\n* Rafal Zielinski                                    0.97850\n  rafal zielinski born 1957 montreal is an independent filmmaker he is best known for direct\n  ing films such as fun sundance film festival special jury award\n==========================================================\nCluster 1    \nleague:0.052\nrugby:0.044\nclub:0.042\ncup:0.042\nseason:0.041\n\n\n* Chris Day                                          0.93220\n  christopher nicholas chris day born 28 july 1975 is an english professional footballer who\n   plays as a goalkeeper for stevenageday started his career at tottenham\n\n* Gary Hooper                                        0.93481\n  gary hooper born 26 january 1988 is an english professional footballer who plays as a forw\n  ard for norwich cityhooper started his career at nonleague grays\n\n* Tony Smith (footballer, born 1957)                 0.93504\n  anthony tony smith born 20 february 1957 is a former footballer who played as a central de\n  fender in the football league in the 1970s and\n\n* Jason Roberts (footballer)                         0.93527\n  jason andre davis roberts mbe born 25 january 1978 is a former professional footballer and\n   now a football punditborn in park royal london roberts was\n\n* Paul Robinson (footballer, born 1979)              0.93587\n  paul william robinson born 15 october 1979 is an english professional footballer who plays\n   for blackburn rovers as a goalkeeper he is a former england\n\n* Alex Lawless                                       0.93732\n  alexander graham alex lawless born 26 march 1985 is a welsh professional footballer who pl\n  ays for luton town as a midfielderlawless began his career with\n\n* Neil Grayson                                       0.93748\n  neil grayson born 1 november 1964 in york is an english footballer who last played as a st\n  riker for sutton towngraysons first club was local\n\n* Sol Campbell                                       0.93759\n  sulzeer jeremiah sol campbell born 18 september 1974 is a former england international foo\n  tballer a central defender he had a 19year career playing in the\n==========================================================\nCluster 2    \nchampionships:0.040\ntour:0.037\nchampionship:0.032\nworld:0.029\nwon:0.029\n\n\n* Alessandra Aguilar                                 0.94505\n  alessandra aguilar born 1 july 1978 in lugo is a spanish longdistance runner who specialis\n  es in marathon running she represented her country in the event\n\n* Heather Samuel                                     0.94529\n  heather barbara samuel born 6 july 1970 is a retired sprinter from antigua and barbuda who\n   specialized in the 100 and 200 metres in 1990\n\n* Viola Kibiwot                                      0.94617\n  viola jelagat kibiwot born december 22 1983 in keiyo district is a runner from kenya who s\n  pecialises in the 1500 metres kibiwot won her first\n\n* Ayelech Worku                                      0.94636\n  ayelech worku born june 12 1979 is an ethiopian longdistance runner most known for winning\n   two world championships bronze medals on the 5000 metres she\n\n* Morhad Amdouni                                     0.94763\n  morhad amdouni born 21 january 1988 in portovecchio is a french middle and longdistance ru\n  nner he was european junior champion in track and cross country\n\n* Krisztina Papp                                     0.94776\n  krisztina papp born 17 december 1982 in eger is a hungarian long distance runner she is th\n  e national indoor record holder over 5000 mpapp began\n\n* Petra Lammert                                      0.94869\n  petra lammert born 3 march 1984 in freudenstadt badenwrttemberg is a former german shot pu\n  tter and current bobsledder she was the 2009 european indoor champion\n\n* Hasan Mahboob                                      0.94880\n  hasan mahboob ali born silas kirui on 31 december 1981 in kapsabet is a bahraini longdista\n  nce runner he became naturalized in bahrain and switched from\n==========================================================\nCluster 3    \nbaseball:0.110\nleague:0.103\nmajor:0.052\ngames:0.047\nseason:0.045\n\n\n* Steve Springer                                     0.89300\n  steven michael springer born february 11 1961 is an american former professional baseball \n  player who appeared in major league baseball as a third baseman and\n\n* Dave Ford                                          0.89547\n  david alan ford born december 29 1956 is a former major league baseball pitcher for the ba\n  ltimore orioles born in cleveland ohio ford attended lincolnwest\n\n* Todd Williams                                      0.89820\n  todd michael williams born february 13 1971 in syracuse new york is a former major league \n  baseball relief pitcher he attended east syracuseminoa high school\n\n* Justin Knoedler                                    0.90035\n  justin joseph knoedler born july 17 1980 in springfield illinois is a former major league \n  baseball catcherknoedler was originally drafted by the st louis cardinals\n\n* Kevin Nicholson (baseball)                         0.90643\n  kevin ronald nicholson born march 29 1976 is a canadian baseball shortstop he played part \n  of the 2000 season for the san diego padres of\n\n* James Baldwin (baseball)                           0.90648\n  james j baldwin jr born july 15 1971 is a former major league baseball pitcher he batted a\n  nd threw righthanded in his 11season career he\n\n* Joe Strong                                         0.90655\n  joseph benjamin strong born september 9 1962 in fairfield california is a former major lea\n  gue baseball pitcher who played for the florida marlins from 2000\n\n* Javier L%C3%B3pez (baseball)                       0.90691\n  javier alfonso lpez born july 11 1977 is a puerto rican professional baseball pitcher for \n  the san francisco giants of major league baseball he is\n==========================================================\nCluster 4    \nresearch:0.038\nuniversity:0.035\nprofessor:0.032\nscience:0.023\ninstitute:0.019\n\n\n* Lawrence W. Green                                  0.95957\n  lawrence w green is best known by health education researchers as the originator of the pr\n  ecede model and codeveloper of the precedeproceed model which has\n\n* Timothy Luke                                       0.96057\n  timothy w luke is university distinguished professor of political science in the college o\n  f liberal arts and human sciences as well as program chair of\n\n* Ren%C3%A9e Fox                                     0.96100\n  rene c fox a summa cum laude graduate of smith college in 1949 earned her phd in sociology\n   in 1954 from radcliffe college harvard university\n\n* Francis Gavin                                      0.96323\n  francis j gavin is first frank stanton chair in nuclear security policy studies and profes\n  sor of political science at mit before joining mit he was\n\n* Catherine Hakim                                    0.96374\n  catherine hakim born 30 may 1948 is a british sociologist who specialises in womens employ\n  ment and womens issues she is currently a professorial research fellow\n\n* Stephen Park Turner                                0.96405\n  stephen turner is a researcher in social practice social and political theory and the phil\n  osophy of the social sciences he is graduate research professor in\n\n* Robert Bates (political scientist)                 0.96489\n  robert hinrichs bates born 1942 is an american political scientist he is eaton professor o\n  f the science of government in the departments of government and\n\n* Georg von Krogh                                    0.96505\n  georg von krogh was born in oslo norway he is a professor at eth zurich and holds the chai\n  r of strategic management and innovation he\n==========================================================\nCluster 5    \nfootball:0.076\ncoach:0.060\nbasketball:0.056\nseason:0.044\nplayed:0.037\n\n\n* Todd Curley                                        0.92731\n  todd curley born 14 january 1973 is a former australian rules footballer who played for co\n  llingwood and the western bulldogs in the australian football league\n\n* Ashley Prescott                                    0.92992\n  ashley prescott born 11 september 1972 is a former australian rules footballer he played w\n  ith the richmond and fremantle football clubs in the afl between\n\n* Pete Richardson                                    0.93204\n  pete richardson born october 17 1946 in youngstown ohio is a former american football defe\n  nsive back in the national football league and former college head\n\n* Nathan Brown (Australian footballer born 1976)     0.93561\n  nathan daniel brown born 14 august 1976 is an australian rules footballer who played for t\n  he melbourne demons in the australian football leaguehe was drafted\n\n* Earl Spalding                                      0.93654\n  earl spalding born 11 march 1965 in south perth is a former australian rules footballer wh\n  o played for melbourne and carlton in the victorian football\n\n* Bud Grant                                          0.93766\n  harry peter bud grant jr born may 20 1927 is a former american football and canadian footb\n  all head coach grant served as the head coach\n\n* Tyrone Wheatley                                    0.93885\n  tyrone anthony wheatley born january 19 1972 is the running backs coach of michigan and a \n  former professional american football player who played 10 seasons\n\n* Nick Salter                                        0.93916\n  nick salter born 30 july 1987 is an australian rules footballer who played for port adelai\n  de football club in the australian football league aflhe was\n==========================================================\nCluster 6    \nshe:0.138\nher:0.089\nactress:0.014\nfilm:0.013\nmiss:0.012\n\n\n* Lauren Royal                                       0.93445\n  lauren royal born march 3 circa 1965 is a book writer from california royal has written bo\n  th historic and novelistic booksa selfproclaimed angels baseball fan\n\n* Barbara Hershey                                    0.93496\n  barbara hershey born barbara lynn herzstein february 5 1948 once known as barbara seagull \n  is an american actress in a career spanning nearly 50 years\n\n* Janet Jackson                                      0.93559\n  janet damita jo jackson born may 16 1966 is an american singer songwriter and actress know\n  n for a series of sonically innovative socially conscious and\n\n* Jane Fonda                                         0.93759\n  jane fonda born lady jayne seymour fonda december 21 1937 is an american actress writer po\n  litical activist former fashion model and fitness guru she is\n\n* Janine Shepherd                                    0.93833\n  janine lee shepherd am born 1962 is an australian pilot and former crosscountry skier shep\n  herds career as an athlete ended when she suffered major injuries\n\n* Ellina Graypel                                     0.93847\n  ellina graypel born july 19 1972 is an awardwinning russian singersongwriter she was born \n  near the volga river in the heart of russia she spent\n\n* Alexandra Potter                                   0.93858\n  alexandra potter born 1970 is a british author of romantic comediesborn in bradford yorksh\n  ire england and educated at liverpool university gaining an honors degree in\n\n* Melissa Hart (actress)                             0.93913\n  melissa hart is an american actress singer and teacher she made her broadway debut in 1966\n   as an ensemble member in jerry bocks the apple\n==========================================================\nCluster 7    \nmusic:0.057\nalbum:0.040\nband:0.035\norchestra:0.023\nreleased:0.022\n\n\n* Brenton Broadstock                                 0.95722\n  brenton broadstock ao born 1952 is an australian composerbroadstock was born in melbourne \n  he studied history politics and music at monash university and later composition\n\n* Prince (musician)                                  0.96057\n  prince rogers nelson born june 7 1958 known by his mononym prince is an american singerson\n  gwriter multiinstrumentalist and actor he has produced ten platinum albums\n\n* Will.i.am                                          0.96066\n  william adams born march 15 1975 known by his stage name william pronounced will i am is a\n  n american rapper songwriter entrepreneur actor dj record\n\n* Tom Bancroft                                       0.96117\n  tom bancroft born 1967 london is a british jazz drummer and composer he began drumming age\n  d seven and started off playing jazz with his father\n\n* Julian Knowles                                     0.96152\n  julian knowles is an australian composer and performer specialising in new and emerging te\n  chnologies his creative work spans the fields of composition for theatre dance\n\n* Dan Siegel (musician)                              0.96223\n  dan siegel born in seattle washington is a pianist composer and record producer his earlie\n  r music has been described as new age while his more\n\n* Tony Mills (musician)                              0.96238\n  tony mills born 7 july 1962 in solihull england is an english rock singer best known for h\n  is work with shy and tnthailing from birmingham\n\n* Don Robertson (composer)                           0.96249\n  don robertson born 1942 is an american composerdon robertson was born in 1942 in denver co\n  lorado and began studying music with conductor and pianist antonia\n==========================================================\nCluster 8    \nhockey:0.216\nnhl:0.134\nice:0.065\nseason:0.053\nleague:0.047\n\n\n* Gord Sherven                                       0.83598\n  gordon r sherven born august 21 1963 in gravelbourg saskatchewan and raised in mankota sas\n  katchewan is a retired canadian professional ice hockey forward who played\n\n* Eric Brewer                                        0.83765\n  eric peter brewer born april 17 1979 is a canadian professional ice hockey defenceman for \n  the anaheim ducks of the national hockey league nhl he\n\n* Stephen Johns (ice hockey)                         0.84580\n  stephen johns born april 18 1992 is an american professional ice hockey defenceman he is c\n  urrently playing with the rockford icehogs of the american hockey\n\n* Mike Stevens (ice hockey, born 1965)               0.85320\n  mike stevens born december 30 1965 in kitchener ontario is a retired professional ice hock\n  ey player who played 23 games in the national hockey league\n\n* Tanner Glass                                       0.85484\n  tanner glass born november 29 1983 is a canadian professional ice hockey winger who plays \n  for the new york rangers of the national hockey league\n\n* Todd Strueby                                       0.86053\n  todd kenneth strueby born june 15 1963 in lanigan saskatchewan and raised in humboldt sask\n  atchewan is a retired canadian professional ice hockey centre who played\n\n* Steven King (ice hockey)                           0.86129\n  steven andrew king born july 22 1969 in east greenwich rhode island is a former ice hockey\n   forward who played professionally from 1991 to 2000\n\n* Don Jackson (ice hockey)                           0.86661\n  donald clinton jackson born september 2 1956 in minneapolis minnesota and bloomington minn\n  esota is an ice hockey coach and a retired professional ice hockey player\n==========================================================\nCluster 9    \nparty:0.028\nelection:0.025\nminister:0.025\nserved:0.021\nlaw:0.019\n\n\n* Doug Lewis                                         0.96516\n  douglas grinslade doug lewis pc qc born april 17 1938 is a former canadian politician a ch\n  artered accountant and lawyer by training lewis entered the\n\n* David Anderson (British Columbia politician)       0.96530\n  david a anderson pc oc born august 16 1937 in victoria british columbia is a former canadi\n  an cabinet minister educated at victoria college in victoria\n\n* Lucienne Robillard                                 0.96679\n  lucienne robillard pc born june 16 1945 is a canadian politician and a member of the liber\n  al party of canada she sat in the house\n\n* Bob Menendez                                       0.96686\n  robert bob menendez born january 1 1954 is the senior united states senator from new jerse\n  y he is a member of the democratic party first\n\n* Mal Sandon                                         0.96706\n  malcolm john mal sandon born 16 september 1945 is an australian politician he was an austr\n  alian labor party member of the victorian legislative council from\n\n* Roger Price (Australian politician)                0.96717\n  leo roger spurway price born 26 november 1945 is a former australian politician he was ele\n  cted as a member of the australian house of representatives\n\n* Maureen Lyster                                     0.96734\n  maureen anne lyster born 10 september 1943 is an australian politician she was an australi\n  an labor party member of the victorian legislative assembly from 1985\n\n* Don Bell                                           0.96739\n  donald h bell born march 10 1942 in new westminster british columbia is a canadian politic\n  ian he is currently serving as a councillor for the\n==========================================================\n"
    }
   ],
   "source": [
    "k = 10\n",
    "visualize_document_clusters(wiki, tf_idf, centroids[k], cluster_assignment[k], k, map_index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters 0, 1, and 5 appear to be still mixed, but others are quite consistent in content.\n",
    "* Cluster 0: artists, book, him/his\n",
    "* Cluster 1: film, theatre, films, tv, actor \n",
    "* Cluster 2: baseball players\n",
    "* Cluster 3: elections, ministers\n",
    "* Cluster 4: music, orchestra, symphony \n",
    "* Cluster 5: female figures from various fields\n",
    "* Cluster 6: composers, songwriters, singers, music producers\n",
    "* Cluster 7: law, courts, justice \n",
    "* Cluster 8: football \n",
    "* Cluster 9: academia\n",
    "\n",
    "Clusters are now more pure, but some are qualitatively \"bigger\" than others. For instance, the category of scholars is more general than the category of baseball players. Increasing the number of clusters may split larger clusters. Another way to look at the size of the clusters is to count the number of articles in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([17602,  3415,  3535,  1736,  6445,  2552,  7106,  7155,   599,\n        8926])"
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "np.bincount(cluster_assignment[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Which of the 10 clusters above contains the greatest number of articles?\n",
    "\n",
    "1. * Cluster 0: artists, book, him/his\n",
    "2. * Cluster 4: music, orchestra, symphony \n",
    "3. * Cluster 5: female figures from various fields\n",
    "4. * Cluster 7: law, courts, justice \n",
    "5. * Cluster 9: academia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Which of the 10 clusters contains the least number of articles?\n",
    "\n",
    "1. * Cluster 1: film, theatre, films, tv, actor \n",
    "2. * Cluster 3: elections, ministers\n",
    "3. * Cluster 6: composers, songwriters, singers, music producers\n",
    "4. * Cluster 7: law, courts, justice \n",
    "5. * Cluster 8: football "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be at least some connection between the topical consistency of a cluster and the number of its member data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the case for K=25. For the sake of brevity, we do not print the content of documents. It turns out that the top words with highest TF-IDF weights in each cluster are representative of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "==========================================================\nCluster 0    \nlaw:0.077\ndistrict:0.048\ncourt:0.046\nrepublican:0.038\nsenate:0.038\n\n==========================================================\nCluster 1    \nresearch:0.054\nprofessor:0.033\nscience:0.032\nuniversity:0.031\nphysics:0.029\n\n==========================================================\nCluster 2    \nhockey:0.216\nnhl:0.134\nice:0.065\nseason:0.052\nleague:0.047\n\n==========================================================\nCluster 3    \nparty:0.065\nelection:0.042\nelected:0.031\nparliament:0.027\nmember:0.023\n\n==========================================================\nCluster 4    \nboard:0.025\npresident:0.023\nchairman:0.022\nbusiness:0.022\nexecutive:0.020\n\n==========================================================\nCluster 5    \nminister:0.160\nprime:0.056\ncabinet:0.044\nparty:0.043\nelection:0.042\n\n==========================================================\nCluster 6    \nuniversity:0.044\nprofessor:0.037\nstudies:0.035\nhistory:0.034\nphilosophy:0.031\n\n==========================================================\nCluster 7    \nelection:0.066\nmanitoba:0.058\nliberal:0.051\nparty:0.045\nriding:0.043\n\n==========================================================\nCluster 8    \nracing:0.095\nformula:0.056\nchampionship:0.054\nrace:0.052\npoker:0.051\n\n==========================================================\nCluster 9    \neconomics:0.146\neconomic:0.096\neconomist:0.053\npolicy:0.048\nresearch:0.043\n\n==========================================================\nCluster 10    \nchampionships:0.075\nolympics:0.050\nmarathon:0.048\nmetres:0.048\nshe:0.048\n\n==========================================================\nCluster 11    \nshe:0.144\nher:0.092\nmiss:0.016\nactress:0.015\ntelevision:0.012\n\n==========================================================\nCluster 12    \nhe:0.011\nradio:0.009\nshow:0.009\nthat:0.009\nhis:0.009\n\n==========================================================\nCluster 13    \nbaseball:0.109\nleague:0.104\nmajor:0.052\ngames:0.047\nseason:0.045\n\n==========================================================\nCluster 14    \nart:0.144\nmuseum:0.076\ngallery:0.056\nartist:0.033\narts:0.031\n\n==========================================================\nCluster 15    \nfootball:0.125\nafl:0.060\nnfl:0.051\nseason:0.049\nplayed:0.045\n\n==========================================================\nCluster 16    \nmusic:0.097\njazz:0.061\npiano:0.033\ncomposer:0.029\norchestra:0.028\n\n==========================================================\nCluster 17    \nleague:0.052\nrugby:0.044\nclub:0.043\ncup:0.042\nseason:0.042\n\n==========================================================\nCluster 18    \npoetry:0.055\nnovel:0.045\nbook:0.042\npublished:0.039\nfiction:0.035\n\n==========================================================\nCluster 19    \nfilm:0.095\ntheatre:0.038\nfilms:0.035\ndirected:0.029\ntelevision:0.028\n\n==========================================================\nCluster 20    \nalbum:0.064\nband:0.049\nmusic:0.037\nreleased:0.033\nsong:0.025\n\n==========================================================\nCluster 21    \nbishop:0.075\nair:0.066\nforce:0.048\nchurch:0.047\ncommand:0.045\n\n==========================================================\nCluster 22    \norchestra:0.146\nopera:0.116\nsymphony:0.106\nconductor:0.077\nmusic:0.064\n\n==========================================================\nCluster 23    \nbasketball:0.120\ncoach:0.105\nnba:0.065\nhead:0.042\nseason:0.040\n\n==========================================================\nCluster 24    \ntour:0.256\npga:0.213\ngolf:0.142\nopen:0.073\ngolfer:0.062\n\n==========================================================\n"
    }
   ],
   "source": [
    "visualize_document_clusters(wiki, tf_idf, centroids[25], cluster_assignment[25], 25,\n",
    "                            map_index_to_word, display_content=False) # turn off text for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the representative examples and top words, we classify each cluster as follows.\n",
    "\n",
    "* Cluster 0: Literature\n",
    "* Cluster 1: Film and theater\n",
    "* Cluster 2: Law\n",
    "* Cluster 3: Politics\n",
    "* Cluster 4: Classical music\n",
    "* Cluster 5: Popular music\n",
    "* Cluster 6: Jazz music\n",
    "* Cluster 7: Business and economics\n",
    "* Cluster 8: (mixed; no clear theme)\n",
    "* Cluster 9: Academia and research\n",
    "* Cluster 10: International affairs\n",
    "* Cluster 11: Baseball\n",
    "* Cluster 12: Art\n",
    "* Cluster 13: Military\n",
    "* Cluster 14: Politics\n",
    "* Cluster 15: Radio and TV\n",
    "* Cluster 16: Catholic church\n",
    "* Cluster 17: Opera and ballet\n",
    "* Cluster 18: Orchestra music\n",
    "* Cluster 19: Females from various fields\n",
    "* Cluster 20: Car racing\n",
    "* Cluster 21: General sports\n",
    "* Cluster 22: Rugby\n",
    "* Cluster 23: Rock music\n",
    "* Cluster 24: Team sports\n",
    "\n",
    "Indeed, increasing K achieved the desired effect of breaking up large clusters.  Depending on the application, this may or may not be preferable to the K=10 analysis.\n",
    "\n",
    "Let's take it to the extreme and set K=100. We have a suspicion that this value is too large. Let us look at the top words from each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "==========================================================\nCluster 0    \nbrazilian:0.137\nbrazil:0.082\nde:0.056\nrio:0.053\npaulo:0.050\n\n==========================================================\nCluster 1    \nbishop:0.170\ndiocese:0.085\narchbishop:0.083\nchurch:0.072\nordained:0.058\n\n==========================================================\nCluster 2    \nzealand:0.247\nnew:0.069\nauckland:0.056\nwellington:0.031\nzealands:0.029\n\n==========================================================\nCluster 3    \ncomics:0.181\ncomic:0.121\nstrip:0.042\ngraphic:0.036\nbook:0.034\n\n==========================================================\nCluster 4    \npuerto:0.309\nrico:0.220\nrican:0.066\njuan:0.041\nricos:0.031\n\n==========================================================\nCluster 5    \nbbc:0.192\nradio:0.127\npresenter:0.054\nshow:0.046\nnews:0.042\n\n==========================================================\nCluster 6    \nsenate:0.059\ndistrict:0.053\ncounty:0.051\ncommittee:0.049\nstate:0.044\n\n==========================================================\nCluster 7    \nlabor:0.105\naustralian:0.099\nliberal:0.071\nelection:0.067\nseat:0.061\n\n==========================================================\nCluster 8    \neconomics:0.065\nuniversity:0.048\nresearch:0.045\nprofessor:0.043\neconomic:0.043\n\n==========================================================\nCluster 9    \nforeign:0.086\nambassador:0.076\naffairs:0.061\nnations:0.053\nunited:0.040\n\n==========================================================\nCluster 10    \nshe:0.188\nher:0.052\nwomen:0.026\nwomens:0.020\ncouncil:0.019\n\n==========================================================\nCluster 11    \nrowing:0.246\nsculls:0.097\nrower:0.081\nolympics:0.073\nchampionships:0.068\n\n==========================================================\nCluster 12    \nfashion:0.086\nphotography:0.085\nphotographer:0.057\nphotographs:0.038\nart:0.025\n\n==========================================================\nCluster 13    \nrepublican:0.098\ngovernor:0.051\ndistrict:0.044\nelection:0.043\nsenate:0.043\n\n==========================================================\nCluster 14    \norchestra:0.227\nsymphony:0.177\nphilharmonic:0.084\nmusic:0.080\nconductor:0.057\n\n==========================================================\nCluster 15    \nair:0.375\nforce:0.242\ncommand:0.106\ncommander:0.094\nbase:0.080\n\n==========================================================\nCluster 16    \nbaseball:0.098\nleague:0.097\nera:0.083\npitcher:0.083\npitched:0.075\n\n==========================================================\nCluster 17    \nchurch:0.114\ntheology:0.072\ntheological:0.066\nseminary:0.047\nchristian:0.037\n\n==========================================================\nCluster 18    \nsong:0.071\nsongs:0.043\nmusic:0.041\nalbum:0.030\nsinger:0.025\n\n==========================================================\nCluster 19    \nbasketball:0.165\nnba:0.113\npoints:0.067\nseason:0.044\nrebounds:0.044\n\n==========================================================\nCluster 20    \nart:0.209\nmuseum:0.186\ngallery:0.082\narts:0.046\ncontemporary:0.044\n\n==========================================================\nCluster 21    \npoetry:0.213\npoems:0.083\npoet:0.069\npoets:0.044\nliterary:0.040\n\n==========================================================\nCluster 22    \nguitar:0.215\nguitarist:0.045\nmusic:0.045\nguitars:0.037\nclassical:0.028\n\n==========================================================\nCluster 23    \nnovel:0.127\npublished:0.045\nnovels:0.044\nbook:0.039\nfiction:0.030\n\n==========================================================\nCluster 24    \njazz:0.205\nmusic:0.048\nband:0.034\npianist:0.025\nrecorded:0.023\n\n==========================================================\nCluster 25    \npolish:0.211\npoland:0.097\nwarsaw:0.091\nsejm:0.039\nshe:0.023\n\n==========================================================\nCluster 26    \ntrinidad:0.259\ntobago:0.178\ncalypso:0.058\ncaribbean:0.033\nsoca:0.027\n\n==========================================================\nCluster 27    \ntour:0.261\npga:0.220\ngolf:0.140\nopen:0.073\ngolfer:0.063\n\n==========================================================\nCluster 28    \nafl:0.177\nfootball:0.128\naustralian:0.092\nadelaide:0.064\nseason:0.062\n\n==========================================================\nCluster 29    \nskating:0.263\nskater:0.107\nspeed:0.095\nshe:0.066\nice:0.060\n\n==========================================================\nCluster 30    \nparty:0.073\nelection:0.035\nelected:0.029\ncandidate:0.022\nparliament:0.021\n\n==========================================================\nCluster 31    \nrugby:0.198\ncup:0.049\nagainst:0.046\nplayed:0.045\nwales:0.040\n\n==========================================================\nCluster 32    \nbook:0.039\nbooks:0.029\npublished:0.026\neditor:0.021\nauthor:0.017\n\n==========================================================\nCluster 33    \npiano:0.150\nmusic:0.071\norchestra:0.056\ncompetition:0.053\npianist:0.051\n\n==========================================================\nCluster 34    \nwrestling:0.299\nwwe:0.163\nwrestler:0.092\nchampionship:0.079\ntag:0.078\n\n==========================================================\nCluster 35    \nopera:0.269\nshe:0.067\nla:0.041\nsang:0.040\noperatic:0.036\n\n==========================================================\nCluster 36    \nradio:0.080\nshow:0.069\nhost:0.038\nsports:0.030\ntelevision:0.028\n\n==========================================================\nCluster 37    \nmusic:0.131\ncomposition:0.038\ncomposer:0.037\norchestra:0.026\nensemble:0.023\n\n==========================================================\nCluster 38    \ndrummer:0.099\nband:0.092\nalbum:0.040\ndrums:0.039\nrock:0.034\n\n==========================================================\nCluster 39    \nmoore:0.306\nmoores:0.034\nher:0.021\nshe:0.020\nsports:0.012\n\n==========================================================\nCluster 40    \ncomputer:0.086\nengineering:0.072\nresearch:0.045\nscience:0.044\ntechnology:0.042\n\n==========================================================\nCluster 41    \nminister:0.164\nprime:0.068\ncabinet:0.043\nparty:0.039\ngovernment:0.038\n\n==========================================================\nCluster 42    \nresearch:0.062\nprofessor:0.035\nuniversity:0.034\nscience:0.031\npsychology:0.030\n\n==========================================================\nCluster 43    \nnews:0.127\nanchor:0.062\nreporter:0.059\nshe:0.045\ncorrespondent:0.045\n\n==========================================================\nCluster 44    \nleague:0.088\ntown:0.060\nseason:0.060\nclub:0.059\nfootball:0.055\n\n==========================================================\nCluster 45    \nfootball:0.046\ncup:0.044\nclub:0.042\nteam:0.041\nleague:0.033\n\n==========================================================\nCluster 46    \nfootball:0.108\nvfl:0.099\naustralian:0.068\nmelbourne:0.067\ngoals:0.064\n\n==========================================================\nCluster 47    \ndesign:0.166\narchitecture:0.119\narchitectural:0.058\narchitects:0.038\narchitect:0.037\n\n==========================================================\nCluster 48    \nphilosophy:0.227\nphilosophical:0.045\nuniversity:0.044\nprofessor:0.041\nphilosopher:0.041\n\n==========================================================\nCluster 49    \nphysics:0.121\nmathematics:0.072\nmathematical:0.060\ntheory:0.053\nprofessor:0.043\n\n==========================================================\nCluster 50    \nbaron:0.070\nlord:0.060\nlords:0.054\nchairman:0.035\nbritish:0.034\n\n==========================================================\nCluster 51    \nchef:0.143\nfood:0.136\nrestaurant:0.095\nwine:0.086\ncooking:0.064\n\n==========================================================\nCluster 52    \nfiction:0.138\nstories:0.069\nshort:0.054\nfantasy:0.048\nwriters:0.043\n\n==========================================================\nCluster 53    \npoker:0.477\nwsop:0.121\nevent:0.091\nlimit:0.078\nwinnings:0.072\n\n==========================================================\nCluster 54    \ncanadian:0.122\ncanada:0.068\ntoronto:0.053\nontario:0.049\ncurling:0.028\n\n==========================================================\nCluster 55    \nsri:0.282\nlanka:0.183\nlankan:0.094\ncolombo:0.046\nceylon:0.027\n\n==========================================================\nCluster 56    \nconductor:0.207\norchestra:0.136\nconducting:0.087\nmusic:0.080\nsymphony:0.073\n\n==========================================================\nCluster 57    \nprison:0.035\npolice:0.027\nsentenced:0.026\ncourt:0.025\nconvicted:0.023\n\n==========================================================\nCluster 58    \nblues:0.234\nband:0.047\nmusic:0.039\nalbum:0.037\nguitar:0.035\n\n==========================================================\nCluster 59    \ndj:0.093\nhop:0.052\nhip:0.051\nmusic:0.048\nalbum:0.037\n\n==========================================================\nCluster 60    \nde:0.127\nla:0.059\nel:0.035\nmexico:0.026\ny:0.025\n\n==========================================================\nCluster 61    \njewish:0.193\nrabbi:0.132\nisrael:0.052\nhebrew:0.038\njews:0.032\n\n==========================================================\nCluster 62    \nballet:0.362\ndance:0.109\ndancer:0.084\nshe:0.057\ndanced:0.044\n\n==========================================================\nCluster 63    \nhockey:0.220\nnhl:0.138\nice:0.067\nseason:0.053\nleague:0.048\n\n==========================================================\nCluster 64    \nlaw:0.148\ncourt:0.093\njudge:0.071\ndistrict:0.051\njustice:0.043\n\n==========================================================\nCluster 65    \ncoach:0.205\nhead:0.086\nbasketball:0.059\ncoaching:0.052\nfootball:0.046\n\n==========================================================\nCluster 66    \narmenian:0.278\narmenia:0.168\nyerevan:0.100\nsargsyan:0.055\ngenocide:0.031\n\n==========================================================\nCluster 67    \nalbum:0.088\nreleased:0.044\nmusic:0.040\nrecords:0.033\nalbums:0.027\n\n==========================================================\nCluster 68    \nshe:0.158\nher:0.152\nmusic:0.020\nalbum:0.016\nsinger:0.013\n\n==========================================================\nCluster 69    \ntheatre:0.194\ndirected:0.034\nproduction:0.031\nplay:0.029\nactor:0.027\n\n==========================================================\nCluster 70    \nhealth:0.099\nmedical:0.089\nmedicine:0.086\nresearch:0.039\nclinical:0.039\n\n==========================================================\nCluster 71    \neuropean:0.145\nparliament:0.115\nparty:0.053\nmember:0.049\ncommittee:0.048\n\n==========================================================\nCluster 72    \nmarathon:0.459\nhalf:0.087\nshe:0.082\nhours:0.063\nchampionships:0.062\n\n==========================================================\nCluster 73    \nshe:0.147\nher:0.105\nactress:0.098\nfilm:0.063\nrole:0.054\n\n==========================================================\nCluster 74    \nshe:0.101\nher:0.065\nwomen:0.012\nshow:0.010\ntelevision:0.009\n\n==========================================================\nCluster 75    \nlds:0.196\nchurch:0.177\nchurchs:0.099\nlatterday:0.074\nbyu:0.073\n\n==========================================================\nCluster 76    \nquebec:0.242\nqubcois:0.064\nuniversit:0.061\nminister:0.059\nparti:0.051\n\n==========================================================\nCluster 77    \nfilm:0.233\nfestival:0.085\nfilms:0.048\ndocumentary:0.048\nfeature:0.045\n\n==========================================================\nCluster 78    \nhong:0.288\nkong:0.268\nchinese:0.068\nchina:0.037\nwong:0.035\n\n==========================================================\nCluster 79    \nsoccer:0.296\nleague:0.072\nindoor:0.065\nteam:0.053\nseason:0.052\n\n==========================================================\nCluster 80    \nhe:0.011\nthat:0.009\nhis:0.009\nworld:0.008\nit:0.007\n\n==========================================================\nCluster 81    \nireland:0.092\nnorthern:0.072\nelection:0.072\nirish:0.066\ngael:0.054\n\n==========================================================\nCluster 82    \ncomedy:0.048\nseries:0.047\nactor:0.043\ntelevision:0.038\nrole:0.037\n\n==========================================================\nCluster 83    \nracing:0.128\nformula:0.080\nrace:0.066\ncar:0.061\ndriver:0.055\n\n==========================================================\nCluster 84    \nelection:0.096\nmanitoba:0.086\nliberal:0.071\nparty:0.067\nconservative:0.060\n\n==========================================================\nCluster 85    \nbusiness:0.038\ncompany:0.031\nchairman:0.027\nceo:0.025\nmanagement:0.023\n\n==========================================================\nCluster 86    \nchess:0.414\ngrandmaster:0.085\nolympiad:0.066\nchampionship:0.064\nfide:0.059\n\n==========================================================\nCluster 87    \ntennis:0.077\ndoubles:0.068\nboxing:0.057\ntitle:0.048\nopen:0.047\n\n==========================================================\nCluster 88    \npresident:0.038\nserved:0.028\nboard:0.028\nuniversity:0.026\neducation:0.022\n\n==========================================================\nCluster 89    \ncampaign:0.061\npresidential:0.054\npolitical:0.047\nrepublican:0.037\nbush:0.037\n\n==========================================================\nCluster 90    \nfootball:0.120\nnfl:0.106\nyards:0.081\nbowl:0.052\nquarterback:0.041\n\n==========================================================\nCluster 91    \nbaseball:0.117\nleague:0.108\nruns:0.061\nmajor:0.052\nbatted:0.044\n\n==========================================================\nCluster 92    \nalbum:0.115\nher:0.073\nbillboard:0.066\nchart:0.064\nsingles:0.064\n\n==========================================================\nCluster 93    \nfilm:0.087\nfilms:0.050\ndirected:0.029\ntelevision:0.024\nactor:0.022\n\n==========================================================\nCluster 94    \nchampionships:0.106\nmetres:0.086\nshe:0.059\nm:0.059\nathletics:0.054\n\n==========================================================\nCluster 95    \nart:0.109\ngallery:0.040\nartist:0.036\npaintings:0.032\npainting:0.032\n\n==========================================================\nCluster 96    \nband:0.120\nalbum:0.040\nbands:0.035\nbass:0.031\nrock:0.030\n\n==========================================================\nCluster 97    \nmiss:0.361\npageant:0.209\nusa:0.127\nshe:0.110\nteen:0.063\n\n==========================================================\nCluster 98    \nfreestyle:0.155\nswimming:0.120\nm:0.119\nswimmer:0.090\nheat:0.075\n\n==========================================================\nCluster 99    \narmy:0.081\ncommander:0.080\ncommand:0.076\nmilitary:0.076\nstaff:0.058\n\n==========================================================\n"
    }
   ],
   "source": [
    "k=100\n",
    "visualize_document_clusters(wiki, tf_idf, centroids[k], cluster_assignment[k], k,\n",
    "                            map_index_to_word, display_content=False)\n",
    "# turn off text for brevity -- turn it on if you are curious ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class of team sports has been broken into several clusters, soccer (association football) (11, 22, 24), rugby (76), hockey (80), basketball (86), cricket (87), and American football (85).\n",
    "\n",
    "The class of baseball has been broken into San Francisco Giants (45), baseball (61, 74), and baseball stats (88).\n",
    "\n",
    "The class of car racing has been broken into Nascar (20) and Formula 1 (52).\n",
    "\n",
    "**A high value of K encourages pure clusters, but we cannot keep increasing K. For large enough K, related documents end up going to different clusters.**\n",
    "\n",
    "That said, the result for K=100 is not entirely bad. After all, it gives us separate clusters for such categories as Brazil, wrestling, computer science and the Mormon Church. If we set K somewhere between 25 and 100, we should be able to avoid breaking up clusters while discovering new ones.\n",
    "\n",
    "Also, we should ask ourselves how much **granularity** we want in our clustering. If we wanted a rough sketch of Wikipedia, we don't want too detailed clusters. On the other hand, having many clusters can be valuable when we are zooming into a certain part of Wikipedia.\n",
    "\n",
    "**There is no golden rule for choosing K. It all depends on the particular application and domain we are in.**\n",
    "\n",
    "Another heuristic people use that does not rely on so much visualization, which can be hard in many applications (including here!) is as follows.  Track heterogeneity versus K and look for the \"elbow\" of the curve where the heterogeneity decrease rapidly before this value of K, but then only gradually for larger values of K.  This naturally trades off between trying to minimize heterogeneity, but reduce model complexity.  In the heterogeneity versus K plot made above, we did not yet really see a flattening out of the heterogeneity, which might indicate that indeed K=100 is \"reasonable\" and we only see real overfitting for larger values of K (which are even harder to visualize using the methods we attempted above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Quiz Question**. Another sign of too large K is having lots of small clusters. Look at the distribution of cluster sizes (by number of member data points). How many of the 100 clusters have fewer than 236 articles, i.e. 0.4% of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "29"
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "sum([1 if x<236 else 0 for x in np.bincount(cluster_assignment[100])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "\n",
    "Keep in mind though that tiny clusters aren't necessarily bad. A tiny cluster of documents that really look like each others is definitely preferable to a medium-sized cluster of documents with mixed content. However, having too few articles in a cluster may cause overfitting by reading too much into a limited pool of training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit (conda)",
   "language": "python",
   "name": "python37664bitconda566ba5c91f704513ab58cb335e9679a8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}